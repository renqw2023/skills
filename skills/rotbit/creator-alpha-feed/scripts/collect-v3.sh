#!/bin/bash
###############################################################################
# AIå†…å®¹æ”¶é›†è„šæœ¬ - V3 æµè§ˆå™¨ç‰ˆ (æ”¯æŒTwitter)
# æ•°æ®æº: Hacker News / Reddit / TechCrunch / Twitter (browseræŠ“å–)
# ç”¨æ³•: ./collect-v3.sh [æ—¥æœŸï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©]
###############################################################################

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PIPELINE_DIR="$(dirname "$SCRIPT_DIR")"
DATE="${1:-$(date +%Y-%m-%d)}"
OUTPUT_DIR="$PIPELINE_DIR/collected/$DATE"
mkdir -p "$OUTPUT_DIR"

LOG_FILE="$OUTPUT_DIR/collection.log"
RAW_FILE="$OUTPUT_DIR/raw-content.json"
MARKDOWN_FILE="$OUTPUT_DIR/raw-content.md"
TWITTER_FILE="$OUTPUT_DIR/twitter-snapshot.html"

log() {
    echo "[$(date '+%H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# åˆå§‹åŒ–JSONå’ŒMarkdown
echo '{"date": "'$DATE'", "sources": []}' > "$RAW_FILE"
cat > "$MARKDOWN_FILE" << EOF
# ğŸ¤– AIå†…å®¹æ”¶é›†æŠ¥å‘Š - $DATE

> â° æ”¶é›†æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')  
> ğŸ“Š æ•°æ®æº: Hacker News | Reddit | TechCrunch | Twitter

---

EOF

log "========== å¼€å§‹æ”¶é›†AIå†…å®¹ =========="
TOTAL=0

###############################################################################
# 1. Hacker News - çƒ­é—¨AIå†…å®¹
###############################################################################
log "ğŸ“¡ [1/4] Hacker News..."

HN_FILE=$(mktemp)
API_URL="https://hn.algolia.com/api/v1/search?query=AI+OR+GPT+OR+LLM&tags=story&numericFilters=points%3E20&hitsPerPage=15"

if curl -s --max-time 30 "$API_URL" -o "$HN_FILE" 2>/dev/null && jq -e '.hits' "$HN_FILE" > /dev/null 2>&1; then
    COUNT=$(jq '.hits | length' "$HN_FILE")
    log "   âœ… è·å–åˆ° $COUNT æ¡"
    
    ITEMS=$(jq '[.hits[] | select(.points >= 10)] | map({
        title: .title,
        url: (.url // ("https://news.ycombinator.com/item?id=" + .objectID)),
        author: .author,
        points: .points,
        comments: .num_comments,
        source: "Hacker News",
        hn_url: ("https://news.ycombinator.com/item?id=" + .objectID)
    })' "$HN_FILE")
    
    ITEM_COUNT=$(echo "$ITEMS" | jq 'length')
    if [[ $ITEM_COUNT -gt 0 ]]; then
        SOURCE_JSON=$(jq -n --arg name "ğŸ”¥ Hacker News" --arg id "hn-ai" --argjson items "$ITEMS" \
            '{name: $name, id: $id, count: ($items | length), items: $items}')
        jq --argjson source "$SOURCE_JSON" '.sources += [$source]' "$RAW_FILE" > "${RAW_FILE}.tmp" && mv "${RAW_FILE}.tmp" "$RAW_FILE"
        
        {
            echo "## ğŸ”¥ Hacker News - çƒ­é—¨AIå†…å®¹"
            echo ""
            echo "$ITEMS" | jq -r '.[] | 
                "### " + .title + "\n" +
                "- ğŸ“Š **çƒ­åº¦**: â¬†ï¸ " + (.points | tostring) + " points | ğŸ’¬ " + (.comments | tostring) + " comments\n" +
                "- ğŸ‘¤ **ä½œè€…**: @" + .author + "\n" +
                "- ğŸ”— **é“¾æ¥**: [åŸæ–‡](" + .url + ") | [HNè®¨è®º](" + .hn_url + ")\n" +
                "---\n"
            '
        } >> "$MARKDOWN_FILE"
        TOTAL=$((TOTAL + ITEM_COUNT))
    fi
else
    log "   âŒ è·å–å¤±è´¥"
fi
rm -f "$HN_FILE"

###############################################################################
# 2. TechCrunch - AIæ¿å— (RSS)
###############################################################################
log "ğŸ“¡ [2/4] TechCrunch..."

TECHCRUNCH_FILE=$(mktemp)
if curl -s --max-time 30 \
    "https://api.rss2json.com/v1/api.json?rss_url=https://techcrunch.com/category/artificial-intelligence/feed/" \
    -o "$TECHCRUNCH_FILE" 2>/dev/null && jq -e '.items' "$TECHCRUNCH_FILE" > /dev/null 2>&1; then
    
    ITEMS=$(jq '.items[:8] | map({
        title: .title,
        url: .link,
        author: .author,
        published: .pubDate,
        description: (.description | gsub("<[^>]+>"; "") | .[:180]),
        source: "TechCrunch AI"
    })' "$TECHCRUNCH_FILE")
    
    ITEM_COUNT=$(echo "$ITEMS" | jq 'length')
    if [[ $ITEM_COUNT -gt 0 ]]; then
        SOURCE_JSON=$(jq -n --arg name "ğŸ“° TechCrunch AI" --arg id "techcrunch-ai" --argjson items "$ITEMS" \
            '{name: $name, id: $id, count: ($items | length), items: $items}')
        jq --argjson source "$SOURCE_JSON" '.sources += [$source]' "$RAW_FILE" > "${RAW_FILE}.tmp" && mv "${RAW_FILE}.tmp" "$RAW_FILE"
        
        {
            echo "## ğŸ“° TechCrunch - AIæ¿å—"
            echo ""
            echo "$ITEMS" | jq -r '.[] | 
                "### " + .title + "\n" +
                "- ğŸ‘¤ **ä½œè€…**: " + .author + " | ğŸ“… " + .published[:10] + "\n" +
                "- ğŸ“ **æ‘˜è¦**: " + .description + "...\n" +
                "- ğŸ”— **é“¾æ¥**: [é˜…è¯»åŸæ–‡](" + .url + ")\n" +
                "---\n"
            '
        } >> "$MARKDOWN_FILE"
        TOTAL=$((TOTAL + ITEM_COUNT))
        log "   âœ… TechCrunch: $ITEM_COUNT æ¡"
    fi
else
    log "   âš ï¸ TechCrunchæš‚æ—¶ä¸å¯ç”¨"
fi
rm -f "$TECHCRUNCH_FILE"

###############################################################################
# 3. Reddit - AIç¤¾åŒº
###############################################################################
log "ğŸ“¡ [3/4] Reddit AIç¤¾åŒº..."

REDDIT_TOTAL=0

# r/ArtificialIntelligence
for SUBREDDIT in "ArtificialIntelligence" "singularity" "LocalLLaMA"; do
    REDDIT_FILE=$(mktemp)
    if curl -s --max-time 25 \
        -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)" \
        "https://www.reddit.com/r/${SUBREDDIT}/top.json?t=day&limit=8" \
        -o "$REDDIT_FILE" 2>/dev/null && jq -e '.data.children' "$REDDIT_FILE" > /dev/null 2>&1; then
        
        ITEMS=$(jq '[.data.children[] | select(.data.stickied != true and .data.ups > 3)] | map({
            title: .data.title,
            url: .data.url,
            permalink: "https://reddit.com" + .data.permalink,
            author: .data.author,
            upvotes: .data.ups,
            comments: .data.num_comments,
            domain: .data.domain,
            source: "Reddit r/'$SUBREDDIT'"
        })' "$REDDIT_FILE")
        
        ITEM_COUNT=$(echo "$ITEMS" | jq 'length')
        if [[ $ITEM_COUNT -gt 0 ]]; then
            SOURCE_JSON=$(jq -n --arg name "ğŸ¤– Reddit r/$SUBREDDIT" --arg id "reddit-$SUBREDDIT" --argjson items "$ITEMS" \
                '{name: $name, id: $id, count: ($items | length), items: $items}')
            jq --argjson source "$SOURCE_JSON" '.sources += [$source]' "$RAW_FILE" > "${RAW_FILE}.tmp" && mv "${RAW_FILE}.tmp" "$RAW_FILE"
            
            if [[ $REDDIT_TOTAL -eq 0 ]]; then
                echo "## ğŸ¤– Reddit - AIç¤¾åŒº" >> "$MARKDOWN_FILE"
                echo "" >> "$MARKDOWN_FILE"
            fi
            
            echo "### r/$SUBREDDIT" >> "$MARKDOWN_FILE"
            echo "" >> "$MARKDOWN_FILE"
            
            echo "$ITEMS" | jq -r '.[] | 
                "#### " + .title + "\n" +
                "- ğŸ“Š â¬†ï¸ " + (.upvotes | tostring) + " | ğŸ’¬ " + (.comments | tostring) + " | ğŸ‘¤ u/" + .author + "\n" +
                "- ğŸ”— [" + .domain + "](" + .url + ") | [Reddit](" + .permalink + ")\n"
            ' >> "$MARKDOWN_FILE"
            
            TOTAL=$((TOTAL + ITEM_COUNT))
            REDDIT_TOTAL=$((REDDIT_TOTAL + ITEM_COUNT))
        fi
    fi
    rm -f "$REDDIT_FILE"
done

if [[ $REDDIT_TOTAL -gt 0 ]]; then
    log "   âœ… Reddit: $REDDIT_TOTAL æ¡"
    echo "---" >> "$MARKDOWN_FILE"
    echo "" >> "$MARKDOWN_FILE"
fi

###############################################################################
# 4. Twitter - é€šè¿‡æµè§ˆå™¨æŠ“å–
###############################################################################
log "ğŸ“¡ [4/4] Twitter (æµè§ˆå™¨æŠ“å–)..."
log "   âš ï¸ Twitteréœ€è¦ä½¿ç”¨OpenClaw browserå·¥å…·æ‰‹åŠ¨æŠ“å–"
log "   è¯·è¿è¡Œ: openclaw agent --message 'æ”¶é›†Twitterçƒ­é—¨AIæ¨æ–‡'"

# åˆ›å»ºTwitteræŠ“å–æŒ‡å—
cat > "$OUTPUT_DIR/twitter-guide.md" << 'EOF'
# Twitter AIå†…å®¹æŠ“å–æŒ‡å—

ç”±äºTwitterçš„åçˆ¬æœºåˆ¶å’Œç™»å½•è¦æ±‚ï¼Œéœ€è¦é€šè¿‡OpenClaw browserå·¥å…·æ‰‹åŠ¨/åŠè‡ªåŠ¨æŠ“å–ã€‚

## æ–¹æ³•1: ä½¿ç”¨ Nitter (æ¨è)

Nitteræ˜¯Twitterçš„å¼€æºé•œåƒï¼Œæ— éœ€ç™»å½•å³å¯è®¿é—®ï¼š

```bash
# è®¿é—®AIç›¸å…³è´¦å·æˆ–æœç´¢
openclaw browser open "https://nitter.net/search?f=tweets&q=AI+OR+GPT+OR+Claude&since=2026-02-09"
```

## æ–¹æ³•2: ä½¿ç”¨æµè§ˆå™¨æŠ“å–Twitterè¶‹åŠ¿

1. æ‰“å¼€Twitteræœç´¢é¡µ
2. æœç´¢çƒ­é—¨AIè¯é¢˜
3. ä½¿ç”¨ snapshot æŠ“å–å†…å®¹

æ¨èå…³æ³¨çš„Twitterè´¦å·ï¼š
- @sama (Sam Altman)
- @gdb (Greg Brockman)
- @ylecun (Yann LeCun)
- @DrJimFan
- @karpathy
- @AndrewYNg
- @ bindu reddy
- @hardmaru

## æ–¹æ³•3: ä½¿ç”¨ Twitter List

åˆ›å»ºä¸€ä¸ªåŒ…å«AIé¢†åŸŸKOLçš„Twitter Listï¼Œå®šæœŸé€šè¿‡browserè®¿é—®ï¼š

```
https://twitter.com/i/lists/YOUR_LIST_ID
```

## è‡ªåŠ¨åŒ–æ–¹æ¡ˆ

æœªæ¥å¯è€ƒè™‘ï¼š
- ä½¿ç”¨ RSSHub ç”ŸæˆTwitter RSS
- ä½¿ç”¨ nitter.net çš„RSSåŠŸèƒ½
- ä½¿ç”¨ä»˜è´¹Twitter API ($100/æœˆèµ·)
EOF

# æ·»åŠ æç¤ºåˆ°MarkdownæŠ¥å‘Š
{
    echo "## ğŸ¦ Twitter - AIçƒ­é—¨æ¨æ–‡"
    echo ""
    echo "âš ï¸ **æ³¨æ„**: Twitterå†…å®¹éœ€è¦é€šè¿‡browserå·¥å…·æ‰‹åŠ¨æŠ“å–"
    echo ""
    echo "æŸ¥çœ‹æŠ“å–æŒ‡å—: \`$OUTPUT_DIR/twitter-guide.md\`"
    echo ""
    echo "**å»ºè®®æ“ä½œ**ï¼ˆåœ¨OpenClawä¸­æ‰§è¡Œï¼‰:"
    echo '```'
    echo '# æ–¹æ³•1: è®¿é—®Nitteræœç´¢AIå†…å®¹'
    echo 'browser open "https://nitter.net/search?f=tweets&q=artificial+intelligence&since=2026-02-09"'
    echo ''
    echo '# æ–¹æ³•2: è®¿é—®ç‰¹å®šè´¦å·æ—¶é—´çº¿'
    echo 'browser open "https://nitter.net/sama"'
    echo ''
    echo '# æ–¹æ³•3: æŠ“å–åä½¿ç”¨web_fetchè·å–é¡µé¢å†…å®¹'
    echo 'web_fetch "https://nitter.net/search?f=tweets&q=GPT"'
    echo '```'
    echo ""
    echo "---"
    echo ""
} >> "$MARKDOWN_FILE"

###############################################################################
# å®Œæˆ
###############################################################################
log "========== æ”¶é›†å®Œæˆ =========="
log "ğŸ“Š æ€»è®¡: $TOTAL æ¡å†…å®¹ (ä¸å«Twitter)"

echo "" >> "$MARKDOWN_FILE"
echo "---" >> "$MARKDOWN_FILE"
echo "" >> "$MARKDOWN_FILE"
echo "ğŸ“Š **æ±‡æ€»**: å…±æ”¶é›† $TOTAL æ¡AIç›¸å…³å†…å®¹" >> "$MARKDOWN_FILE"
echo "- Hacker News: çƒ­é—¨AIé¡¹ç›®" >> "$MARKDOWN_FILE"
echo "- TechCrunch: AIæ–°é—»" >> "$MARKDOWN_FILE"
echo "- Reddit: ç¤¾åŒºè®¨è®º" >> "$MARKDOWN_FILE"
echo "- Twitter: éœ€æ‰‹åŠ¨æŠ“å– (è§twitter-guide.md)" >> "$MARKDOWN_FILE"

echo ""
echo "âœ… æ”¶é›†å®Œæˆ! å…± $TOTAL æ¡ (ä¸å«Twitter)"
echo ""
echo "ğŸ“„ ä¸»æŠ¥å‘Š: $MARKDOWN_FILE"
echo "ğŸ“– TwitteræŒ‡å—: $OUTPUT_DIR/twitter-guide.md"
echo ""
echo "ğŸ’¡ è¦æŠ“å–Twitterå†…å®¹ï¼Œè¯·åœ¨OpenClawä¸­è¿è¡Œ:"
echo "   browser open 'https://nitter.net/search?f=tweets&q=AI'"
