#!/usr/bin/env python3
"""
å°é¥­å¡ - å¤§ä¼—ç‚¹è¯„æœç´¢
ç”¨æ³•:
  python3 search.py "ä¸‰é‡Œå±¯ åˆ›æ„èœ"
  python3 search.py "å›½è´¸ æ—¥æ–™ äººå‡500" --city åŒ—äº¬
  python3 search.py "æœé˜³ ç´ é£Ÿ" --max 15 --json
"""

import sys
import json
import argparse
import re
import os


PROXY = os.environ.get('DDGS_PROXY') or None


def search_dianping(query: str, city: str = '', max_results: int = 20) -> list:
    """æœç´¢å¤§ä¼—ç‚¹è¯„ä¸Šçš„é¤å…ä¿¡æ¯"""
    from ddgs import DDGS
    ddgs = DDGS(proxy=PROXY)

    city_str = f' {city}' if city else ''
    search_queries = [
        f'site:dianping.com {query}{city_str} é¤å…',
        f'{query}{city_str} é¤å… å¤§ä¼—ç‚¹è¯„ æ¨è',
    ]

    all_results = []
    seen_urls = set()

    for sq in search_queries:
        try:
            results = ddgs.text(sq, max_results=max_results, region='cn-zh')
            for r in results:
                url = r.get('href', '')
                if url not in seen_urls:
                    seen_urls.add(url)
                    all_results.append(r)
        except Exception as e:
            print(f"æœç´¢å‡ºé”™ [{sq}]: {e}", file=sys.stderr)

    restaurants = []
    for r in all_results:
        parsed = parse_result(r)
        if parsed:
            restaurants.append(parsed)

    return restaurants


def parse_result(result):
    """ä»æœç´¢ç»“æœä¸­è§£æé¤å…ä¿¡æ¯"""
    title = result.get('title', '')
    body = result.get('body', '')
    url = result.get('href', '')
    combined = f"{title} {body}"

    is_dianping = 'dianping.com' in url
    food_keywords = ['é¤å…', 'é¤é¦†', 'é¥­åº—', 'èœ', 'äººå‡', 'æ¨èèœ', 'å¥½åƒ', 'å‘³é“', 'æ–™ç†', 'ç«é”…', 'çƒ¤', 'åº—', 'é¦†']
    is_restaurant = any(kw in combined for kw in food_keywords)

    if not is_restaurant:
        return None

    # è¿‡æ»¤éé¤å…é¡µé¢
    skip_patterns = ['shopRank', 'pcChannelRanking', '/photos', '/album']
    if any(p in url for p in skip_patterns):
        return None

    non_food = ['æŒ‰æ‘©', 'è¶³æµ´', 'å…»ç”Ÿé¦†', 'ç¾å®¹', 'ç¾å‘', 'é…’åº—', 'KTV', 'å¥èº«']
    if any(nf in combined for nf in non_food) and not any(kw in combined for kw in food_keywords[:6]):
        return None

    # æå–äººå‡
    price_match = re.search(r'[äººå‡Â¥ï¿¥](\d+)', combined)
    avg_price = int(price_match.group(1)) if price_match else None

    # æå–åº—å
    shop_name = None
    name_match = re.search(r'ã€(.+?)ã€‘', title)
    if name_match:
        shop_name = name_match.group(1)
    elif '(' in title and 'å¤§ä¼—ç‚¹è¯„' not in title:
        shop_name = title.split(' - ')[0].split('|')[0].strip()

    # æå–è¯„åˆ†
    score_match = re.search(r'(\d\.\d)\s*åˆ†', combined)
    score = float(score_match.group(1)) if score_match else None

    # æå–èœç³»
    categories = []
    cat_keywords = {
        'ä¸­é¤': ['ä¸­é¤', 'äº¬èœ', 'é²èœ', 'å·èœ', 'ç²¤èœ', 'æ¹˜èœ', 'æµ™èœ', 'è‹èœ', 'å¾½èœ', 'é—½èœ'],
        'æ—¥æ–™': ['æ—¥æœ¬æ–™ç†', 'æ—¥æ–™', 'å¯¿å¸', 'åˆºèº«', 'å±…é…’å±‹', 'omakase'],
        'è¥¿é¤': ['è¥¿é¤', 'æ³•é¤', 'æ„å¤§åˆ©', 'è¥¿ç­ç‰™', 'ç‰›æ’'],
        'ç«é”…': ['ç«é”…', 'æ¶®è‚‰', 'æ¶®é”…'],
        'çƒ§çƒ¤': ['çƒ¤è‚‰', 'çƒ§çƒ¤', 'ç‚™å­'],
        'ç´ é£Ÿ': ['ç´ é£Ÿ', 'ç´ èœ', 'è”¬é£Ÿ'],
        'åˆ›æ„èœ': ['åˆ›æ„èœ', 'èåˆèœ', 'æ–°æ´¾'],
        'ç§æˆ¿èœ': ['ç§æˆ¿èœ', 'ç§å¨'],
        'ä¸œå—äºš': ['æ³°å›½èœ', 'è¶Šå—èœ', 'ä¸œå—äºš', 'æ³°é¤'],
        'éŸ©é¤': ['éŸ©å›½æ–™ç†', 'éŸ©é¤', 'éŸ©å›½èœ'],
        'æ½®æ±•': ['æ½®æ±•', 'æ½®å·', 'æ±•å¤´'],
        'äº‘å—èœ': ['äº‘å—èœ', 'æ»‡èœ', 'äº‘è´µ'],
        'è´µå·èœ': ['è´µå·èœ', 'é»”èœ'],
    }
    for cat, keywords in cat_keywords.items():
        if any(kw in combined for kw in keywords):
            categories.append(cat)

    is_shop_page = '/shop/' in url or '/shopshare/' in url

    return {
        'name': shop_name or title[:40],
        'avg_price': avg_price,
        'score': score,
        'categories': categories,
        'snippet': body[:200] if body else '',
        'url': url,
        'source': 'dianping' if is_dianping else 'search',
        'is_shop_page': is_shop_page,
    }


def main():
    parser = argparse.ArgumentParser(description='å°é¥­å¡ - å¤§ä¼—ç‚¹è¯„æœç´¢')
    parser.add_argument('query', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--city', default='', help='åŸå¸‚')
    parser.add_argument('--max', type=int, default=20, help='æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--json', action='store_true', help='JSONè¾“å‡º')
    args = parser.parse_args()

    restaurants = search_dianping(args.query, args.city, args.max)

    if args.json:
        print(json.dumps(restaurants, ensure_ascii=False, indent=2))
    else:
        if not restaurants:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³é¤å…")
            return

        restaurants.sort(key=lambda x: (x['is_shop_page'], x['avg_price'] is not None), reverse=True)

        print(f"ğŸ” å¤§ä¼—ç‚¹è¯„æœç´¢: {args.query}\n")
        for i, r in enumerate(restaurants, 1):
            name = r['name']
            price = f"Â¥{r['avg_price']}" if r['avg_price'] else ''
            score = f"â­{r['score']}" if r['score'] else ''
            cats = ' '.join(f'#{c}' for c in r['categories']) if r['categories'] else ''
            info = ' | '.join(p for p in [price, score, cats] if p)

            print(f"{i}. {name}")
            if info:
                print(f"   {info}")
            if r['snippet']:
                print(f"   {r['snippet'][:80]}")
            print()


if __name__ == '__main__':
    main()
