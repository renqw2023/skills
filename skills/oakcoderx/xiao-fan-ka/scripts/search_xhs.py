#!/usr/bin/env python3
"""
å°é¥­å¡ - å°çº¢ä¹¦æ¢åº—æœç´¢
é€šè¿‡æœç´¢å¼•æ“æŠ“å–å°çº¢ä¹¦ä¸Šçš„æ¢åº—ç¬”è®°ã€‚
ç”¨æ³•:
  python3 search_xhs.py "ä¸‰é‡Œå±¯ å®è—é¤å…"
  python3 search_xhs.py "å›½è´¸ æ—¥æ–™" --max 10 --json
"""

import sys
import json
import argparse
import re
import os


PROXY = os.environ.get('DDGS_PROXY') or None


def search_xiaohongshu(query: str, max_results: int = 10) -> list:
    """æœç´¢å°çº¢ä¹¦æ¢åº—ç¬”è®°"""
    from ddgs import DDGS
    ddgs = DDGS(proxy=PROXY)

    search_queries = [
        f'site:xiaohongshu.com {query} æ¢åº—',
        f'site:xiaohongshu.com {query} é¤å… æ¨è',
        f'å°çº¢ä¹¦ {query} æ¢åº— å¥½åƒ',
    ]

    all_results = []
    seen_urls = set()

    for sq in search_queries:
        try:
            results = ddgs.text(sq, max_results=max_results, region='cn-zh')
            for r in results:
                url = r.get('href', '')
                if url not in seen_urls:
                    seen_urls.add(url)
                    all_results.append(r)
        except Exception as e:
            print(f"æœç´¢å‡ºé”™ [{sq}]: {e}", file=sys.stderr)

    notes = []
    for r in all_results:
        parsed = parse_xhs_result(r)
        if parsed:
            notes.append(parsed)

    return notes


def parse_xhs_result(result):
    """è§£æå°çº¢ä¹¦æœç´¢ç»“æœ"""
    title = result.get('title', '')
    body = result.get('body', '')
    url = result.get('href', '')
    combined = f"{title} {body}"

    is_xhs = 'xiaohongshu.com' in url

    # åˆ¤æ–­æ˜¯å¦é¤å…ç›¸å…³
    food_keywords = ['é¤å…', 'æ¢åº—', 'å¥½åƒ', 'ç¾é£Ÿ', 'èœ', 'é¦†', 'æ–™ç†', 'æ‰“å¡', 'å¿…åƒ', 'æ¨è', 'äººå‡']
    if not any(kw in combined for kw in food_keywords):
        return None

    # æå–æåˆ°çš„é¤å…åï¼ˆé€šå¸¸åœ¨æ ‡é¢˜æˆ–æ­£æ–‡ä¸­ä»¥ä¹¦åå·æ ‡æ³¨ï¼‰
    restaurant_names = re.findall(r'[ã€Œã€ã€ã€Š](.+?)[ã€ã€ã€‘ã€‹]', combined)

    # æå–äººå‡
    price_match = re.search(r'[äººå‡Â¥ï¿¥](\d+)', combined)
    avg_price = int(price_match.group(1)) if price_match else None

    # åˆ¤æ–­æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
    positive_words = ['å¥½åƒ', 'æ¨è', 'ç»äº†', 'æƒŠè‰³', 'å®è—', 'ç¥ä»™', 'å¿…åƒ', 'å›è´­', 'è¶…èµ', 'æ»¡åˆ†', 'çˆ±äº†']
    negative_words = ['è¸©é›·', 'ä¸å¥½åƒ', 'æ‹”è‰', 'å¤±æœ›', 'éš¾åƒ', 'ä¸æ¨è', 'ä¸€èˆ¬', 'é¿é›·', 'ç¿»è½¦']
    
    pos_count = sum(1 for w in positive_words if w in combined)
    neg_count = sum(1 for w in negative_words if w in combined)
    
    if neg_count > pos_count:
        sentiment = 'negative'
    elif pos_count > 0:
        sentiment = 'positive'
    else:
        sentiment = 'neutral'

    return {
        'title': title[:60],
        'restaurants_mentioned': restaurant_names[:3],
        'avg_price': avg_price,
        'sentiment': sentiment,
        'snippet': body[:200] if body else '',
        'url': url,
        'source': 'xiaohongshu' if is_xhs else 'search',
    }


def main():
    parser = argparse.ArgumentParser(description='å°é¥­å¡ - å°çº¢ä¹¦æœç´¢')
    parser.add_argument('query', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--max', type=int, default=10, help='æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--json', action='store_true', help='JSONè¾“å‡º')
    args = parser.parse_args()

    notes = search_xiaohongshu(args.query, args.max)

    if args.json:
        print(json.dumps(notes, ensure_ascii=False, indent=2))
    else:
        if not notes:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¢åº—ç¬”è®°")
            return

        sentiment_emoji = {'positive': 'ğŸ‘', 'negative': 'ğŸ‘', 'neutral': 'ğŸ˜'}
        print(f"ğŸ“• å°çº¢ä¹¦æ¢åº—: {args.query}\n")
        for i, n in enumerate(notes, 1):
            emoji = sentiment_emoji.get(n['sentiment'], '')
            price = f" Â¥{n['avg_price']}" if n['avg_price'] else ''
            restaurants = f" â†’ {', '.join(n['restaurants_mentioned'])}" if n['restaurants_mentioned'] else ''

            print(f"{i}. {emoji} {n['title']}{price}{restaurants}")
            if n['snippet']:
                print(f"   {n['snippet'][:80]}")
            print(f"   {n['url']}")
            print()


if __name__ == '__main__':
    main()
