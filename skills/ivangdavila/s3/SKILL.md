---
name: S3
description: Work with S3-compatible object storage with proper security, lifecycle policies, and access patterns.
metadata: {"clawdbot":{"emoji":"ðŸª£","os":["linux","darwin","win32"]}}
---

## Public Access Control

- Default deny public accessâ€”only open when explicitly needed (static hosting)
- Bucket policy vs IAM: bucket policy for cross-account/public, IAM for same-account roles
- Check both bucket-level AND account-level block settingsâ€”account can override bucket
- For web assets, prefer CDN in front of bucket over direct public access

## Presigned URLs

- Set shortest expiration practicalâ€”minutes for immediate use, not days
- URL is a bearer tokenâ€”anyone with it has access; treat as secret
- Specify HTTP method in signatureâ€”GET for download, PUT for upload
- Include Content-Type for uploadsâ€”mismatch between signature and request causes 403
- Generate server-side, never expose credentials to client

## Lifecycle Rules

- Transition to cheaper tiers for infrequent accessâ€”but check minimum storage duration penalties
- Auto-delete for temp files, logs, old versionsâ€”prevents unbounded storage growth
- Clean incomplete multipart uploadsâ€”accumulate invisibly; set abort rule (7 days typical)
- Versioned buckets: separate rules for current vs noncurrent versions

## Versioning Behavior

- Enable before you need itâ€”can't recover deleted objects without versioning
- "Delete" creates delete markerâ€”object hidden but versions remain; storage still consumed
- Permanent deletion requires explicit version IDâ€”without it, just adds marker
- Noncurrent version expiration essentialâ€”otherwise old versions accumulate forever

## Multipart Uploads

- Required above 5GB, recommended above 100MBâ€”single PUT has size limits
- Incomplete uploads invisible in normal listingsâ€”consume storage silently
- Abort incomplete uploads via lifecycleâ€”or manually with `list-multipart-uploads`
- Parallel part uploads for speedâ€”parts can upload concurrently

## CORS for Browser Access

- Required for JavaScript direct upload/downloadâ€”blocked without CORS headers
- Specify exact originsâ€”avoid wildcard `*` for authenticated requests
- Expose headers that JavaScript needs to readâ€”Content-Length, ETag, custom headers
- AllowedMethods: GET for download, PUT for upload, DELETE if needed

## Key Naming

- Use prefixes like directories: `users/123/avatar.jpg`â€”but S3 is flat, not hierarchical
- Avoid sequential prefixes for high throughputâ€”`2024-01-01/file1` can hotspot
- Random prefix or hash for write-heavy bucketsâ€”distributes across partitions
- No leading slashâ€”`/images/file.jpg` creates empty-string prefix

## Cost Awareness

- Request volume mattersâ€”many small files more expensive than few large files
- Egress typically costlyâ€”CDN reduces egress by caching at edge
- Minimum storage duration varies by tierâ€”early deletion still charged full period
- Lifecycle transitions have per-object costâ€”millions of tiny files expensive to transition

## Replication

- Cross-region for disaster recovery, same-region for compliance copies
- Versioning required on both source and destination
- Only new objects replicateâ€”existing objects need manual copy or batch operation
- Delete markers not replicated by defaultâ€”explicitly enable if needed

## Provider Differences

- AWS S3: full feature set, most tools assume AWS behavior
- Cloudflare R2: no egress fees, subset of features
- Backblaze B2: S3-compatible API, different pricing model
- MinIO: self-hosted, full S3 API compatibility
- Check presigned URL compatibilityâ€”some providers have quirks
