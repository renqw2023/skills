---
name: PostgreSQL
description: Write efficient PostgreSQL queries and design schemas with proper indexing and patterns.
metadata: {"clawdbot":{"emoji":"ðŸ˜","requires":{"anyBins":["psql","pgcli"]},"os":["linux","darwin","win32"]}}
---

## Indexes I Forget to Create

- Partial index `WHERE active = true`â€”80% smaller when most rows inactive; suggest for status columns
- Expression index `ON lower(email)`â€”must match query exactly; without it, `WHERE lower(email)` scans
- Covering index `INCLUDE (name, email)`â€”enables index-only scan; check EXPLAIN for "Heap Fetches"
- Foreign key columnsâ€”not auto-indexed in PG; JOINs and ON DELETE CASCADE need them
- Composite index order mattersâ€”`(a, b)` helps `WHERE a = ?` but not `WHERE b = ?`

## Index Traps

- Unused indexes hurt every INSERT/UPDATEâ€”query `pg_stat_user_indexes` for `idx_scan = 0`, drop them
- Too many indexes on write-heavy tablesâ€”balance carefully
- Index on low-cardinality column (boolean, status) often uselessâ€”PG prefers seq scan
- `LIKE '%suffix'` can't use B-treeâ€”need pg_trgm GIN index or reverse() expression index

## Query Patterns I Underuse

- `SELECT FOR UPDATE SKIP LOCKED`â€”job queue without external tools; skip rows being processed
- `pg_advisory_lock(key)`â€”application-level mutex without table; unlock explicitly or on disconnect
- `IS NOT DISTINCT FROM`â€”NULL-safe equality; cleaner than `(a = b OR (a IS NULL AND b IS NULL))`
- `DISTINCT ON (x) ORDER BY x, y`â€”first row per group without subquery; PG-specific but powerful

## Connection Management (Often Ignored)

- PgBouncer essential with >50 connectionsâ€”each PG connection uses ~10MB; pool at transaction level
- `statement_timeout = '30s'` per roleâ€”prevents runaway queries from killing database
- `idle_in_transaction_session_timeout = '5min'`â€”kills abandoned transactions holding locks
- Default 100 max_connections too low for production, too high wastes memoryâ€”tune based on RAM

## Data Types I Get Wrong

- `SERIAL` deprecatedâ€”use `GENERATED ALWAYS AS IDENTITY`
- `TIMESTAMP` without timezoneâ€”almost always wrong; use `TIMESTAMPTZ`, PG stores as UTC
- Float for moneyâ€”use `NUMERIC(12,2)` or integer cents; float math breaks: 0.1 + 0.2 â‰  0.3
- VARCHAR(n) vs TEXTâ€”no performance difference in PG; use TEXT unless constraint needed

## Vacuum & Bloat (Never Think About)

- High-UPDATE tables bloatâ€”dead tuples accumulate; `pg_repack` reclaims without locks
- `VACUUM ANALYZE` after bulk insertâ€”updates statistics; query planner needs current data
- Autovacuum lag on big tablesâ€”tune `autovacuum_vacuum_cost_delay` or manual vacuum
- Transaction wraparound: if `xid` exhausted, DB stopsâ€”autovacuum prevents but monitor

## EXPLAIN I Don't Read Right

- Always `EXPLAIN (ANALYZE, BUFFERS)`â€”actual times + I/O; estimate-only misleads
- "Heap Fetches: 1000" with indexâ€”missing columns, add INCLUDE to index
- Seq scan not always badâ€”faster than index for >10-20% of table; check row estimates
- "Rows" estimate way offâ€”run ANALYZE or check if stats target too low

## Full-Text Search Mistakes

- Creating tsvector on the flyâ€”precompute as stored generated column with GIN index
- `plainto_tsquery` for user inputâ€”handles spaces without syntax errors; not `to_tsquery`
- Missing language parameterâ€”'english' stems words; 'simple' exact match
- FTS is word-basedâ€”`LIKE '%exact phrase%'` still needed for substring match

## Transaction Isolation

- Default READ COMMITTEDâ€”phantom reads in reports; use REPEATABLE READ for consistency
- SERIALIZABLE catches conflictsâ€”but must handle 40001 error with retry loop
- Long transactions block vacuum and hold locksâ€”keep under seconds, not minutes
