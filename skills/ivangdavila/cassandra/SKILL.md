---
name: Cassandra
description: Design Cassandra tables, write efficient queries, and avoid distributed database pitfalls.
metadata: {"clawdbot":{"emoji":"ðŸ‘ï¸","requires":{"anyBins":["cqlsh","nodetool"]},"os":["linux","darwin","win32"]}}
---

## Data Modeling Mistakes

- Design tables around queries, not entitiesâ€”denormalization is mandatory, not optional
- One table per query patternâ€”Cassandra has no JOINs; duplicate data across tables
- Partition key determines data distributionâ€”all rows with same partition key on same node
- Wide partitions kill performanceâ€”keep under 100MB; add time bucket to partition key if growing

## Primary Key Traps

- `PRIMARY KEY (a, b, c)`: `a` is partition key, `b` and `c` are clustering columns
- `PRIMARY KEY ((a, b), c)`: `(a, b)` together is partition keyâ€”compound partition key
- Clustering columns define sort order within partitionâ€”query must respect this order
- Can't query by clustering column without partition keyâ€”unlike SQL indexes

## Query Restrictions

- `WHERE` must include full partition keyâ€”partial partition key fails unless `ALLOW FILTERING`
- `ALLOW FILTERING` scans all nodesâ€”never use in production; redesign table instead
- Range queries only on last clustering column usedâ€”`WHERE a = ? AND b > ?` works, `WHERE a = ? AND c > ?` doesn't
- `IN` on partition key hits multiple nodesâ€”expensive; prefer single partition queries

## Consistency Levels

- `QUORUM` for most operationsâ€”majority of replicas; balances consistency and availability
- `LOCAL_QUORUM` for multi-datacenterâ€”avoids cross-DC latency
- `ONE` for pure availabilityâ€”may read stale data; fine for caches, bad for critical reads
- Write + read consistency must overlap for strong consistencyâ€”`QUORUM` + `QUORUM` safe

## Tombstones (Silent Performance Killer)

- DELETE creates a tombstone, not actual deletionâ€”tombstones persist until compaction
- Mass deletes destroy read performanceâ€”thousands of tombstones scanned per query
- TTL also creates tombstonesâ€”don't use short TTLs with high write volume
- Check with `nodetool cfstats -H table`â€”`Tombstone` columns show problem

## Batch Misuse

- UNLOGGED BATCH is not fasterâ€”use only for atomic writes to same partition
- LOGGED BATCH for multi-partition atomicityâ€”adds coordination overhead
- Don't batch unrelated writesâ€”hurts coordinator; send individual async writes
- Batch size limit ~50KBâ€”larger batches fail or timeout

## Anti-Patterns

- Secondary indexes on high-cardinality columnsâ€”scatter-gather query, slow
- Secondary indexes on frequently updated columnsâ€”creates tombstones
- `SELECT *`â€”always list columns; schema changes break queries
- UUID as partition key without time componentâ€”random distribution, hot spots during bulk loads

## Lightweight Transactions

- `IF NOT EXISTS` / `IF column = ?`â€”uses Paxos, 4x slower than normal write
- Serial consistency for LWTsâ€”`SERIAL` or `LOCAL_SERIAL`
- Don't use for counters or high-frequency updatesâ€”contention kills throughput
- Returns `[applied]` booleanâ€”must check if operation succeeded

## Collections and Counters

- Sets/Lists/Maps stored with rowâ€”can't exceed 64KB, no pagination
- List prepend is anti-patternâ€”creates tombstones; use append or Set
- Counters require dedicated tableâ€”can't mix with regular columns
- Counter increment is not idempotentâ€”retry may double-count

## Compaction Strategies

- `SizeTieredCompactionStrategy` (default)â€”good for write-heavy, uses more disk space
- `LeveledCompactionStrategy`â€”better read latency, higher write amplification
- `TimeWindowCompactionStrategy`â€”for time-series with TTL; reduces tombstone overhead
- Wrong strategy for workload = degraded performance over time

## Operations

- `nodetool repair` regularlyâ€”inconsistencies accumulate without repair
- `nodetool status` shows cluster healthâ€”UN (Up Normal) is good, DN is down
- Schema changes propagate eventuallyâ€”wait for `nodetool describecluster` to show agreement
- Rolling restarts: one node at a time, wait for UN status before next
