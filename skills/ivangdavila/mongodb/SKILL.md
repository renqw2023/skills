---
name: MongoDB
description: Design schemas, write queries, and configure MongoDB for consistency and performance.
metadata: {"clawdbot":{"emoji":"ðŸƒ","requires":{"anyBins":["mongosh","mongo"]},"os":["linux","darwin","win32"]}}
---

## Schema Design Decisions

- Embed when data is queried together and doesn't grow unboundedly
- Reference when data is large, accessed independently, or many-to-many
- Arrays that grow infinitely = disasterâ€”document size limit 16MB; use bucketing pattern
- Denormalize for read performance, accept update complexityâ€”no JOINs means duplicate data

## Array Pitfalls

- Arrays > 1000 elements hurt performanceâ€”pagination inside documents is hard
- `$push` without `$slice` = unbounded growth; use `$push: {$each: [...], $slice: -100}`
- Multikey indexes on arrays: index entry per elementâ€”can explode index size
- Can't have multikey index on more than one array field in compound index

## $lookup Is Not a JOIN

- `$lookup` performance degrades with collection sizeâ€”no index usage on foreign collection (until 5.0)
- One `$lookup` per pipeline stageâ€”nested lookups get complex and slow
- Consider embedding or application-side joins for frequent lookups
- `$lookup` with pipeline (5.0+) can filter before joiningâ€”massive performance improvement

## Index Strategy

- ESR rule for compound indexes: Equality fields first, Sort fields next, Range fields last
- MongoDB doesn't do efficient index intersectionâ€”single compound index often better than multiple
- Only one text index per collectionâ€”plan carefully; use Atlas Search for complex text
- TTL index for auto-expiration: `{createdAt: 1}, {expireAfterSeconds: 86400}`

## Aggregation Pipeline

- Stage order matters: `$match` and `$project` early to reduce documents flowing through
- `$match` at start can use indexes; `$match` after `$unwind` or `$lookup` cannot
- `allowDiskUse: true` for large aggregationsâ€”without it, 100MB memory limit per stage
- `$facet` for multiple aggregations in one queryâ€”but all facets process same documents

## Consistency & Transactions

- Default read/write concern not fully consistentâ€”`{w: "majority", readConcern: "majority"}` for strong consistency
- Multi-document transactions since 4.0â€”but add latency and lock overhead; design to minimize
- Single-document operations are atomicâ€”exploit this by embedding related data
- `retryWrites: true` in connection stringâ€”handles transient failures

## ObjectId Behavior

- Contains timestamp: `ObjectId.getTimestamp()`â€”can extract creation time
- Roughly time-orderedâ€”can sort by `_id` for creation order
- Not randomâ€”predictable if you know creation time and machine; don't rely on for security
- 12 bytes: 4 timestamp + 5 random + 3 counter

## Explain & Performance

- `explain("executionStats")` shows actual executionâ€”not just plan
- Look for `COLLSCAN`â€”means no index used; add appropriate index
- `totalDocsExamined` vs `nReturned`â€”ratio should be close to 1; otherwise index missing
- Covered queries: `IXSCAN` + `"totalDocsExamined": 0`â€”all data from index

## Document Size

- 16MB max per documentâ€”plan for this; use GridFS for large files
- BSON overhead: field names repeated per documentâ€”short names save space at scale
- Nested depth limit 100 levelsâ€”rarely hit but exists

## Read Preferences

- `primary` for strong consistency; `secondaryPreferred` for read scaling with eventual consistency
- Stale reads on secondariesâ€”replication lag can be seconds
- `nearest` for lowest latencyâ€”but may read stale data
- Write always goes to primaryâ€”read preference doesn't affect writes

## Common Mistakes

- Treating MongoDB as "schemaless"â€”still need schema design; just enforced in app
- Not adding indexesâ€”scans entire collection; every query pattern needs index
- Giant documents via array pushesâ€”hit 16MB limit or slow BSON parsing
- Ignoring write concernâ€”data may appear written but not persisted/replicated
