---
name: TensorFlow
description: Avoid common TensorFlow mistakes â€” tf.function retracing, GPU memory, data pipeline bottlenecks, and gradient traps.
metadata: {"clawdbot":{"emoji":"ðŸ§ ","requires":{"bins":["python3"]},"os":["linux","darwin","win32"]}}
---

## tf.function Retracing
- New input shape/dtype causes retrace â€” expensive, prints warning
- Use `input_signature` for fixed shapes â€” `@tf.function(input_signature=[tf.TensorSpec(...)])`
- Python values retrace â€” pass as tensors, not Python ints/floats
- Avoid Python side effects in tf.function â€” only runs once during tracing

## GPU Memory
- TensorFlow grabs all GPU memory by default â€” set `memory_growth=True` before any ops
- `tf.config.experimental.set_memory_growth(gpu, True)` â€” must be called before GPU init
- OOM with large models â€” reduce batch size or use gradient checkpointing
- `CUDA_VISIBLE_DEVICES=""` to force CPU â€” for testing without GPU

## Data Pipeline
- `tf.data.Dataset` without `.prefetch()` â€” CPU/GPU idle time between batches
- `.cache()` after expensive ops â€” but before random augmentation
- `.batch()` before `.map()` for vectorized ops â€” faster than per-element
- `num_parallel_calls=tf.data.AUTOTUNE` â€” parallel preprocessing
- Dataset iteration in eager mode is slow â€” use in tf.function or model.fit

## Shape Issues
- First dimension is batch â€” `None` for variable batch size in Input layer
- `model.build(input_shape)` if not using Input layer â€” or first call errors
- Reshape errors unclear â€” `tf.debugging.assert_shapes()` for debugging
- Broadcasting silently succeeds â€” may hide shape bugs

## Gradient Tape
- Variables watched by default â€” tensors need `tape.watch(tensor)`
- `persistent=True` for multiple gradients â€” otherwise tape consumed after first use
- `tape.gradient` returns None if no path â€” check for disconnected graph
- `@tf.custom_gradient` for custom backward â€” not all ops have gradients

## Training Gotchas
- `model.trainable = False` after compile does nothing â€” set before compile
- BatchNorm behaves differently in training vs inference â€” `training=True/False` matters
- `model.fit` shuffles by default â€” `shuffle=False` for time series
- `validation_split` takes from end â€” shuffle data first if order matters

## Saving Models
- `model.save()` saves everything â€” architecture, weights, optimizer state
- `model.save_weights()` only weights â€” need model code to restore
- SavedModel format for serving â€” `tf.saved_model.save(model, path)`
- H5 format limited â€” doesn't save custom objects well, use SavedModel

## Common Mistakes
- Mixing Keras and raw tf ops incorrectly â€” use `layers.Lambda` to wrap tf ops in Sequential
- `tf.print` vs Python print â€” Python print only runs at trace time in tf.function
- NumPy ops in graph â€” use tf ops, numpy executes eagerly only
- Loss returns scalar per sample â€” Keras averages, custom loops may need `tf.reduce_mean`
