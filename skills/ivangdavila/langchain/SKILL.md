---
name: LangChain
description: Avoid common LangChain mistakes â€” LCEL gotchas, memory persistence, RAG chunking, and output parser traps.
metadata: {"clawdbot":{"emoji":"ðŸ¦œ","requires":{"bins":["python3"]},"os":["linux","darwin","win32"]}}
---

## LCEL Basics
- `|` pipes output to next â€” `prompt | llm | parser`
- `RunnablePassthrough()` forwards input unchanged â€” use in parallel branches
- `RunnableParallel` runs branches concurrently â€” `{"a": chain1, "b": chain2}`
- `.invoke()` for single, `.batch()` for multiple, `.stream()` for tokens
- Input must match expected keys â€” `{"question": x}` not just `x` if prompt expects `{question}`

## Memory Gotchas
- Memory doesn't auto-persist between sessions â€” save/load explicitly
- `ConversationBufferMemory` grows unbounded â€” use `ConversationSummaryMemory` for long chats
- Memory key must match prompt variable â€” `memory_key="chat_history"` needs `{chat_history}` in prompt
- `return_messages=True` for chat models â€” `False` returns string for completion models

## RAG Chunking
- Chunk size affects retrieval quality â€” too small loses context, too large dilutes relevance
- Chunk overlap prevents cutting mid-sentence â€” 10-20% overlap typical
- `RecursiveCharacterTextSplitter` preserves structure â€” splits on paragraphs, then sentences
- Embedding dimension must match vector store â€” mixing models causes silent failures

## Output Parsers
- `PydanticOutputParser` needs format instructions in prompt â€” call `.get_format_instructions()`
- Parser failures aren't always loud â€” malformed JSON may partially parse
- `OutputFixingParser` retries with LLM â€” wraps another parser, fixes errors
- `with_structured_output()` on chat models â€” cleaner than manual parsing for supported models

## Retrieval
- `similarity_search` returns documents â€” `.page_content` for text
- `k` parameter controls results count â€” more isn't always better, noise increases
- Metadata filtering before similarity â€” `filter={"source": "docs"}` in most vector stores
- `max_marginal_relevance_search` for diversity â€” avoids redundant similar chunks

## Agents
- Agents decide tool order dynamically â€” chains are fixed sequence
- Tool descriptions matter â€” agent uses them to decide when to call
- `handle_parsing_errors=True` â€” prevents crash on malformed agent output
- Max iterations prevents infinite loops â€” `max_iterations=10` default may be too low

## Common Mistakes
- Prompt template variables case-sensitive â€” `{Question}` â‰  `{question}`
- Chat models need message format â€” `ChatPromptTemplate`, not `PromptTemplate`
- Callbacks not propagating â€” pass `config={"callbacks": [...]}` through chain
- Rate limits crash silently sometimes â€” wrap in retry logic
- Token count exceeds context â€” use `trim_messages` or summarization for long histories
