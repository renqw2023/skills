---
name: Models
description: Choose AI models for coding, reasoning, and agents with cost-aware, task-matched recommendations.
metadata: {"clawdbot":{"emoji":"ðŸ¤–","os":["linux","darwin","win32"]}}
---

# AI Model Selection Rules

## Core Principle
- No single model is best for everything â€” match model to task, not brand loyalty
- A $0.75/M model often performs identically to a $40/M model for simple tasks
- Test cheaper alternatives before committing to expensive defaults

## Cost Reality
- Output tokens cost 3-10x more than input tokens â€” advertised input prices are misleading
- Calculate real cost with your actual input/output ratio, not theoretical pricing
- Batch/async APIs offer 50% discounts â€” use them for non-real-time workloads
- Prompt caching reduces repeated context costs significantly

## Task Matching

### Coding
- Architecture and design decisions: Use frontier models (Opus-class) â€” they catch subtle issues cheaper models miss
- Day-to-day implementation: Mid-tier models (Sonnet-class) offer 90% of capability at 20% of cost
- Parallel subtasks and scaffolding: Fast/cheap models (Haiku-class) â€” speed matters more than depth
- Code review: Thorough models catch async bugs and edge cases that fast models miss

### Non-Coding
- Complex reasoning and math: Extended thinking modes justify their cost for hard problems
- General assistance: User preference studies favor models different from benchmark leaders
- High-volume simple queries: Cheapest models perform identically â€” don't overpay
- Long documents: Context window size determines viability â€” some offer 1M+ tokens

## Claude Code vs Codex CLI
- Claude Code: Fast iteration, UI/frontend, interactive debugging â€” developer stays in the loop
- Codex CLI: Long-running background tasks, large refactors, set-and-forget â€” accuracy over speed
- Both tools have value â€” use Claude Code for implementation, Codex for final review
- File size limits differ â€” Claude Code struggles with files over 25K tokens

## Orchestration Pattern
- Planning phase: Use expensive/smart models to break down problems correctly
- Execution phase: Use balanced models, parallelize where possible
- Review phase: Use accurate models for final verification â€” catches bugs others miss
- This pattern beats using one model for everything at similar total cost

## Benchmark Skepticism
- Benchmark scores vary 2-3x based on scaffolding and evaluation method
- User preference rankings differ significantly from benchmark rankings
- SWE-bench scores don't predict real-world coding quality reliably
- Models drift week-to-week â€” last month's best may underperform today

## Open Source Viability
- DeepSeek and similar models approach frontier performance at 1/50th API cost
- Self-hosting eliminates API rate limits and price variability
- MIT/Apache licensed models allow commercial use without restrictions
- Consider for: data privacy, cost predictability, custom fine-tuning

## Model Selection Mistakes
- Using premium models for chatbot responses that cheap models handle identically
- Ignoring context window limits â€” chunking long documents costs more than using large-context models
- Expecting consistency â€” same prompt gives different results over time as models update
- Trusting speed over accuracy for complex tasks â€” fast models trade thoroughness for latency

## Practical Guidelines
- Default to mid-tier for most tasks, escalate to frontier only when quality suffers
- Track actual costs per workflow, not just per-token rates
- Build verification into pipelines â€” don't trust any model blindly
- Reassess model choices quarterly â€” pricing and capabilities shift constantly
