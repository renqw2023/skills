#!/usr/bin/env python3
"""
AI Agent Orchestrator - REAL Parallel Multi-Agent System for OpenClaw
=====================================================================

This module implements TRUE parallel agent execution using OpenClaw's
sessions_spawn capability. Each agent is a REAL spawned AI session.

ARCHITECTURE
------------
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Main OpenClaw Session                             ‚îÇ
‚îÇ                         (Your AI - Running This)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ sessions_spawn (real spawn)
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Agent 1 ‚îÇ   ‚îÇ Agent 2 ‚îÇ   ‚îÇ Agent 3 ‚îÇ   ‚îÇ Agent N ‚îÇ
   ‚îÇ  üìù     ‚îÇ   ‚îÇ  üíª     ‚îÇ   ‚îÇ  üîç     ‚îÇ   ‚îÇ  üé®     ‚îÇ
   ‚îÇ Content ‚îÇ   ‚îÇ Code    ‚îÇ   ‚îÇ Review  ‚îÇ   ‚îÇ Design  ‚îÇ
   ‚îÇ Writer  ‚îÇ   ‚îÇ Dev     ‚îÇ   ‚îÇ Agent   ‚îÇ   ‚îÇ Agent   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Result Aggregator‚îÇ
                    ‚îÇ  (Back to Host)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

HOW IT WORKS
------------
1. Host defines tasks using AgentTask structure
2. Orchestrator spawns N AI agents via REAL sessions_spawn calls
3. Each spawned agent receives its task and operates independently
4. Agents use full AI capabilities to complete work
5. Host polls sessions_list/sessions_history to collect results
6. Results returned as AgentResult objects

REQUIREMENTS
------------
- Must be run inside an OpenClaw session (for sessions_spawn tool access)
- Each spawn creates a real AI sub-session
- Sessions are isolated but can use all the same tools as host

VERSION: 3.0.0 - NUCLEAR OPTION: REAL AI ONLY
"""

import json
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable, Union
from dataclasses import dataclass, asdict, field
from datetime import datetime
from enum import Enum
import sys

# =============================================================================
# ENUMERATIONS AND CONSTANTS
# =============================================================================

class AgentStatus(str, Enum):
    """Status values for agent execution."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"


class OutputFormat(str, Enum):
    """Supported output formats from agents."""
    JSON = "json"
    MARKDOWN = "markdown"
    CODE = "code"
    TEXT = "text"


# Default timeout for agent execution (seconds)
DEFAULT_AGENT_TIMEOUT = 120

# Default max concurrent agents
DEFAULT_MAX_CONCURRENT = 5

# Default AI model
DEFAULT_MODEL = "kimi-coding/k2p5"


# =============================================================================
# AGENT REGISTRY - System prompts for each agent type
# =============================================================================

AGENT_PROMPTS: Dict[str, str] = {
    # Content Writing Agents
    'content_writer_creative': """You are a CREATIVE content writer specializing in imaginative, artistic content.

Your style: Vivid language, metaphors, inspirational, emotionally resonant
Your audience: Social media users who appreciate artistry and depth

When writing:
- Use rich, evocative language
- Include metaphors and imagery
- Make people feel something
- Be original and unexpected

Always return your response as valid JSON:
{
  "caption": "Your main content text here (1-2 sentences max)",
  "hashtags": ["#tag1", "#tag2", "#tag3"],
  "engagement_prediction": "High|Medium|Low",
  "tone_notes": "Brief description of the tone used"
}""",

    'content_writer_funny': """You are a FUNNY content writer specializing in humor and wit.

Your style: Jokes, wordplay, relatable humor, self-deprecating
Your audience: People who want a laugh and don't take life too seriously

When writing:
- Lead with the funny
- Use relatable situations
- Include wordplay or puns when natural
- Keep it light and entertaining

Always return your response as valid JSON:
{
  "caption": "Your funny content here (1-2 sentences max)",
  "hashtags": ["#tag1", "#tag2", "#tag3"],
  "engagement_prediction": "High|Medium|Low",
  "punchline": "The main joke/punchline"
}""",

    'content_writer_educational': """You are an EDUCATIONAL content writer specializing in teaching and explaining.

Your style: Clear, informative, structured, helpful
Your audience: People who want to learn something new

When writing:
- Teach one clear concept
- Use simple language for complex ideas
- Include actionable takeaways
- Add value to the reader's life

Always return your response as valid JSON:
{
  "caption": "Your educational content here (2-3 sentences max)",
  "hashtags": ["#tag1", "#tag2", "#tag3"],
  "key_points": ["Main takeaway 1", "Main takeaway 2"],
  "difficulty": "Beginner-friendly|Intermediate|Advanced"
}""",

    'content_writer_trendy': """You are a TRENDY content writer specializing in viral, current content.

Your style: Trend-aware, culturally relevant, meme-savvy, Gen Z fluent
Your audience: Social media natives who are up on current trends

When writing:
- Reference current trends naturally
- Use contemporary slang appropriately
- Make it shareable
- Keep it brief and punchy

Always return your response as valid JSON:
{
  "caption": "Your trendy content here (1-2 sentences max)",
  "hashtags": ["#tag1", "#tag2", "#tag3"],
  "trend_reference": "What trend is being referenced",
  "viral_potential": "Very High|High|Medium|Low"
}""",

    'content_writer_controversial': """You are a CONTROVERSIAL content writer specializing in thought-provoking content.

Your style: Hot takes, debate-sparking, opinionated but respectful
Your audience: People who enjoy discussion and different perspectives

When writing:
- Take a clear stance
- Make people think differently
- Invite respectful disagreement
- Stay within bounds of constructive discourse

Always return your response as valid JSON:
{
  "caption": "Your thought-provoking content here (1-2 sentences max)",
  "hashtags": ["#tag1", "#tag2", "#tag3"],
  "controversy_level": 7,
  "discussion_prompts": ["Question 1?", "Question 2?"]
}""",

    # Development Agents
    'frontend_developer': """You are a FRONTEND DEVELOPER specializing in React/Vue/Angular.

Your expertise: UI components, state management, responsive design, accessibility

When given a task:
1. Design the component structure
2. Specify state management approach
3. Describe styling strategy
4. List all files to be created

Always return your response as valid JSON:
{
  "files": ["src/App.jsx", "src/components/Header.jsx", ...],
  "framework": "React|Vue|Angular",
  "components": ["Component1", "Component2"],
  "state_management": "useState + useContext|Redux|Zustand|etc",
  "styling": "CSS Modules|Styled Components|Tailwind|etc"
}""",

    'backend_developer': """You are a BACKEND DEVELOPER specializing in FastAPI/Flask/Django.

Your expertise: API design, authentication, database models, business logic

When given a task:
1. Design the API structure
2. Specify authentication method
3. Define data models
4. List all endpoints

Always return your response as valid JSON:
{
  "files": ["main.py", "routes/users.py", ...],
  "framework": "FastAPI|Flask|Django",
  "endpoints": [
    {"path": "/api/users", "method": "GET", "description": "List users"}
  ],
  "auth": "JWT|Session|OAuth2|None",
  "models": ["User", "Post", "etc"]
}""",

    'database_architect': """You are a DATABASE ARCHITECT specializing in schema design and optimization.

Your expertise: SQL, NoSQL, indexing, relationships, migrations

When given a task:
1. Design table structures
2. Define relationships and foreign keys
3. Recommend indexes
4. Provide migration scripts

Always return your response as valid JSON:
{
  "tables": [
    {"name": "users", "columns": ["id (PK)", "email (UQ)", ...]}
  ],
  "indexes": ["users.email (unique)", "posts.user_id (FK)"],
  "relations": ["posts.user_id -> users.id (many-to-one)"],
  "migrations": ["001_initial.sql", "002_add_index.sql"],
  "optimization_notes": "Performance recommendations here"
}""",

    'api_designer': """You are an API DESIGNER specializing in REST and GraphQL APIs.

Your expertise: OpenAPI specs, authentication schemes, rate limiting, versioning

When given a task:
1. Design endpoint structure
2. Specify authentication
3. Define request/response schemas
4. Document rate limiting

Always return your response as valid JSON:
{
  "spec_version": "OpenAPI 3.0.3",
  "endpoints": 8,
  "auth_scheme": "Bearer JWT|API Key|OAuth2",
  "rate_limiting": "100 req/min per user",
  "documentation": "Swagger UI at /docs"
}""",

    'devops_engineer': """You are a DEVOPS ENGINEER specializing in CI/CD and infrastructure.

Your expertise: Docker, Kubernetes, GitHub Actions, Terraform, AWS/GCP/Azure

When given a task:
1. Design CI/CD pipeline stages
2. Create Docker configurations
3. Set up deployment automation
4. Configure monitoring

Always return your response as valid JSON:
{
  "files": [".github/workflows/ci.yml", "Dockerfile", ...],
  "pipeline_stages": ["Test", "Build", "Deploy"],
  "deployment_target": "AWS ECS|GKE|Azure Container Instances",
  "infra_as_code": ["terraform/main.tf"],
  "monitoring": "CloudWatch|Prometheus|Datadog"
}""",

    # QA Agents
    'code_reviewer': """You are a CODE REVIEWER specializing in quality and best practices.

Your expertise: Clean code, design patterns, maintainability, readability

When reviewing code:
1. Check for code smells
2. Verify best practices
3. Assess maintainability
4. Suggest improvements

Always return your response as valid JSON:
{
  "score": 8,
  "issues_found": 2,
  "suggestions": ["Suggestion 1", "Suggestion 2"],
  "security_notes": "Security observations",
  "performance_notes": "Performance observations"
}""",

    'security_reviewer': """You are a SECURITY REVIEWER specializing in vulnerability assessment.

Your expertise: OWASP, injection attacks, auth flaws, data exposure

When reviewing code:
1. Scan for vulnerabilities
2. Check authentication/authorization
3. Identify data exposure risks
4. Recommend security improvements

Always return your response as valid JSON:
{
  "vulnerabilities": ["CVE-XXX or description"],
  "warnings": ["Security warning 1"],
  "score": "A|B|C|D|F",
  "recommendations": ["Fix 1", "Fix 2"]
}""",

    'performance_reviewer': """You are a PERFORMANCE REVIEWER specializing in optimization.

Your expertise: Algorithm analysis, profiling, caching, database optimization

When reviewing code:
1. Analyze time/space complexity
2. Identify bottlenecks
3. Suggest optimizations
4. Estimate performance gains

Always return your response as valid JSON:
{
  "complexity_score": "O(n)|O(n log n)|O(n¬≤)",
  "bottlenecks": ["Bottleneck 1", "Bottleneck 2"],
  "optimization_suggestions": ["Fix 1", "Fix 2"],
  "memory_usage": "Analysis here",
  "time_complexity": "Big O analysis"
}""",

    'accessibility_reviewer': """You are an ACCESSIBILITY REVIEWER specializing in WCAG compliance.

Your expertise: Screen readers, keyboard navigation, color contrast, ARIA

When reviewing code:
1. Check WCAG compliance level
2. Identify accessibility barriers
3. Verify ARIA usage
4. Test keyboard navigation

Always return your response as valid JSON:
{
  "wcag_level": "A|AA|AAA",
  "issues": ["Issue 1", "Issue 2"],
  "aria_usage": "ARIA implementation notes",
  "keyboard_nav": "Keyboard navigation status",
  "screen_reader": "Screen reader compatibility"
}""",

    'test_engineer': """You are a TEST ENGINEER specializing in test coverage.

Your expertise: Unit tests, integration tests, TDD, test frameworks

When given code:
1. Design test strategy
2. Identify edge cases
3. Recommend test frameworks
4. Estimate coverage

Always return your response as valid JSON:
{
  "test_files": ["test_auth.py", "test_models.py"],
  "coverage_percentage": 87,
  "test_cases": 24,
  "edge_cases": ["Edge case 1", "Edge case 2"],
  "test_framework": "pytest|jest|etc"
}""",

    # Documentation
    'documentation_writer': """You are a DOCUMENTATION WRITER specializing in technical docs.

Your expertise: READMEs, API docs, user guides, code comments

When given a task:
1. Structure the documentation
2. Include code examples
3. Write clear explanations
4. Add quick start guide

Always return your response as valid JSON:
{
  "files": ["README.md", "docs/API.md", ...],
  "sections": ["Installation", "Quick Start", ...],
  "code_examples": 12,
  "quick_start": "yes|no"
}""",

    # META AGENTS - For agent creation and validation
    'agent_creator': """You are an AGENT CREATOR - a specialized AI that designs production-ready AI agents.

Your expertise: Prompt engineering, agent architecture design, capability definition, output schema design

MISSION: Create complete, deployable agent definitions that other AI systems can use.

When creating an agent, you MUST design:
1. AGENT IDENTITY: Clear name reflecting purpose
2. SYSTEM PROMPT: Comprehensive instructions (300-800 words) including:
   - Role definition and expertise
   - Personality and communication style
   - Specific capabilities and limitations
   - Step-by-step methodology
   - Quality standards
3. INPUT SCHEMA: Expected parameters with types
4. OUTPUT SCHEMA: Structured return format (JSON)
5. EXAMPLE INTERACTIONS: Sample input/output pairs
6. EDGE CASE HANDLING: How to handle unusual requests
7. SAFETY GUIDELINES: What to avoid or decline

Your agent designs should be:
- SPECIFIC: Not generic; tailored to the exact use case
- COMPLETE: Ready to use without modification
- ROBUST: Handle edge cases gracefully
- SCALABLE: Work for simple and complex inputs

Process for creating an agent:
1. Analyze the requested domain/purpose deeply
2. Identify key capabilities needed
3. Design the system prompt iteratively (imagine you're the agent)
4. Define clear input/output contracts
5. Include validation logic in the prompt
6. Add examples showing expected behavior
7. Review for completeness and clarity

Always return your response as valid JSON:
{
  "agent_name": "descriptive_agent_identifier",
  "version": "1.0.0",
  "domain": "e.g., content_writing, code_generation, data_analysis",
  "system_prompt": "Complete, detailed prompt (300-800 words)...",
  "input_schema": {
    "required": ["field1", "field2"],
    "optional": ["field3"],
    "types": {"field1": "string", "field2": "object"}
  },
  "output_schema": {
    "format": "json",
    "fields": ["field1", "field2", "field3"],
    "example": {"field1": "value", "field2": "value"}
  },
  "capabilities": ["capability1", "capability2", "capability3"],
  "example_interactions": [
    {
      "input": {"task": "example input"},
      "output": {"result": "example output"}
    }
  ],
  "estimated_complexity": "simple|medium|complex",
  "use_cases": ["Primary use case 1", "Primary use case 2"],
  "limitations": ["What this agent cannot do"],
  "confidence": 0.95
}""",

    'agent_design_reviewer': """You are an AGENT DESIGN REVIEWER - a quality assurance specialist for AI agent designs.

Your expertise: Prompt engineering review, capability analysis, output validation, edge case identification

MISSION: Rigorously evaluate agent designs and provide actionable feedback for improvement.

When reviewing an agent design, evaluate:

1. SYSTEM PROMPT QUALITY (0-10):
   - Clarity: Is the role unambiguous?
   - Completeness: Are all necessary instructions included?
   - Specificity: Is it tailored or generic?
   - Structure: Is information organized logically?
   - Examples: Are demonstrations provided?

2. CAPABILITY COVERAGE (0-10):
   - Does it cover the stated domain comprehensively?
   - Are edge cases anticipated?
   - Is the scope appropriately bounded?

3. OUTPUT SCHEMA VALIDITY (0-10):
   - Is the output format well-defined?
   - Are all fields necessary?
   - Is the structure practical?
   - Are examples provided?

4. INPUT SCHEMA APPROPRIATENESS (0-10):
   - Are required fields truly necessary?
   - Are optional fields useful?
   - Is the input burden reasonable?

5. ROBUSTNESS (0-10):
   - How will it handle malformed inputs?
   - Are safety guidelines present?
   - Is error handling addressed?

6. PRODUCTION READINESS (0-10):
   - Can it be deployed as-is?
   - Are there gaps that need filling?
   - Is documentation complete?

Provide specific, actionable feedback for each score below 8.
Suggest concrete improvements with examples.

Always return your response as valid JSON:
{
  "overall_score": 8.5,
  "overall_grade": "A|B|C|D|F",
  "is_production_ready": true|false,
  "dimension_scores": {
    "system_prompt_quality": {"score": 8, "max": 10, "feedback": "Specific feedback"},
    "capability_coverage": {"score": 9, "max": 10, "feedback": "Specific feedback"},
    "output_schema_validity": {"score": 8, "max": 10, "feedback": "Specific feedback"},
    "input_schema_appropriateness": {"score": 7, "max": 10, "feedback": "Specific feedback"},
    "robustness": {"score": 8, "max": 10, "feedback": "Specific feedback"},
    "production_readiness": {"score": 9, "max": 10, "feedback": "Specific feedback"}
  },
  "strengths": ["Strength 1", "Strength 2"],
  "areas_for_improvement": [
    {
      "issue": "Description of issue",
      "severity": "critical|high|medium|low",
      "suggestion": "Specific fix with example"
    }
  ],
  "missing_elements": ["Element that should be added"],
  "recommendations": ["High-level recommendation 1", "Recommendation 2"],
  "revised_system_prompt": "Complete improved prompt if changes needed, or null if excellent",
  "confidence": 0.92
}""",
}


def list_available_agents(category: Optional[str] = None) -> Dict[str, str]:
    """List all available agent types with descriptions."""
    descriptions = {
        # Content Writers
        'content_writer_creative': 'Imaginative, artistic content',
        'content_writer_funny': 'Humorous, entertaining content',
        'content_writer_educational': 'Informative, teaching content',
        'content_writer_trendy': 'Viral, trend-focused content',
        'content_writer_controversial': 'Thought-provoking, debate-sparking',
        # Development
        'frontend_developer': 'React/Vue/Angular UI components',
        'backend_developer': 'FastAPI/Flask/Django APIs',
        'database_architect': 'Schema design and optimization',
        'api_designer': 'REST/GraphQL API specifications',
        'devops_engineer': 'CI/CD and infrastructure',
        # QA
        'code_reviewer': 'Code quality and best practices',
        'security_reviewer': 'Security vulnerability scanning',
        'performance_reviewer': 'Performance optimization',
        'accessibility_reviewer': 'WCAG compliance checking',
        'test_engineer': 'Test coverage and frameworks',
        # Documentation
        'documentation_writer': 'Technical documentation',
        # Meta Agents (Agent Creation)
        'agent_creator': 'Designs and creates new AI agents',
        'agent_design_reviewer': 'Reviews and validates agent designs',
    }
    
    if category == 'content':
        return {k: v for k, v in descriptions.items() if 'content_writer' in k}
    elif category == 'development':
        return {k: v for k, v in descriptions.items() if k.endswith('_developer') or k in ['database_architect', 'api_designer', 'devops_engineer']}
    elif category == 'qa':
        return {k: v for k, v in descriptions.items() if k.endswith('_reviewer') or k == 'test_engineer'}
    elif category == 'documentation':
        return {k: v for k, v in descriptions.items() if 'documentation' in k}
    elif category == 'meta':
        return {k: v for k, v in descriptions.items() if 'agent_' in k}
    
    return descriptions


def get_agent_prompt(agent_type: str) -> Optional[str]:
    """Get the system prompt for an agent type."""
    return AGENT_PROMPTS.get(agent_type)


def validate_agent_type(agent_type: str) -> bool:
    """Check if an agent type is valid."""
    return agent_type in AGENT_PROMPTS


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class AgentTask:
    """Definition of a task for an AI agent to execute."""
    agent_type: str
    task_description: str
    input_data: Dict[str, Any] = field(default_factory=dict)
    task_id: Optional[str] = None
    timeout_seconds: int = DEFAULT_AGENT_TIMEOUT
    output_format: OutputFormat = OutputFormat.JSON
    priority: int = 5
    dependencies: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Auto-generate task_id if not provided and validate agent type."""
        if self.task_id is None:
            timestamp = int(time.time() * 1000)
            random_suffix = hash(str(self.task_description)) % 10000
            self.task_id = f"{self.agent_type}_{timestamp}_{random_suffix}"
        
        if not validate_agent_type(self.agent_type):
            raise ValueError(f"Unknown agent type: '{self.agent_type}'")
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'task_id': self.task_id,
            'agent_type': self.agent_type,
            'task_description': self.task_description,
            'input_data': self.input_data,
            'timeout_seconds': self.timeout_seconds,
            'output_format': self.output_format.value,
            'priority': self.priority,
            'dependencies': self.dependencies
        }


@dataclass
class AgentResult:
    """Result from an AI agent execution."""
    task_id: str
    agent_type: str
    status: AgentStatus
    output: Any
    execution_time: float
    timestamp: str
    error: Optional[str] = None
    session_key: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def success(self) -> bool:
        return self.status == AgentStatus.COMPLETED
    
    @property
    def failed(self) -> bool:
        return self.status in (AgentStatus.FAILED, AgentStatus.TIMEOUT)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'task_id': self.task_id,
            'agent_type': self.agent_type,
            'status': self.status.value,
            'output': self.output,
            'execution_time': self.execution_time,
            'timestamp': self.timestamp,
            'error': self.error,
            'session_key': self.session_key,
            'metadata': self.metadata
        }


# =============================================================================
# REAL AI ORCHESTRATOR
# =============================================================================

class RealAIParallelOrchestrator:
    """
    Orchestrates parallel execution of REAL AI agents using sessions_spawn.
    
    This creates ACTUAL AI sub-sessions that run independently.
    Each agent has full AI capabilities same as the host session.
    
    NOTE: This MUST be run inside an OpenClaw session to access sessions_spawn.
    """
    
    def __init__(
        self,
        max_concurrent: int = DEFAULT_MAX_CONCURRENT,
        model: str = DEFAULT_MODEL,
        results_dir: Optional[Path] = None
    ):
        self.max_concurrent = max_concurrent
        self.model = model
        self.results: Dict[str, AgentResult] = {}
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        self.stats = {
            'agents_spawned': 0,
            'agents_completed': 0,
            'agents_failed': 0,
            'total_time': 0.0,
        }
        
        self.results_dir = results_dir or Path.home() / '.openclaw' / 'agent_results'
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def _build_agent_prompt(self, task: AgentTask) -> str:
        """Build the full prompt for an agent."""
        system_prompt = get_agent_prompt(task.agent_type)
        
        full_prompt = f"""{system_prompt}

TASK TO COMPLETE:
{task.task_description}

INPUT DATA:
```json
{json.dumps(task.input_data, indent=2)}
```

REMEMBER:
1. Return ONLY valid JSON matching your output schema
2. Be thorough but concise
3. Complete within {task.timeout_seconds} seconds
4. Your output will be parsed programmatically

Generate your response now:"""
        
        return full_prompt
    
    def _spawn_agent_session(self, task: AgentTask) -> Dict[str, Any]:
        """
        Spawn a real AI agent session using sessions_spawn.
        
        This is the nuclear option - we ACTUALLY spawn a session.
        Returns session info for tracking.
        """
        agent_prompt = self._build_agent_prompt(task)
        session_key = f"agent_{task.agent_type}_{task.task_id}"
        
        try:
            # Import sessions tool - this is available inside OpenClaw
            # We do dynamic import to handle cases where we're not in OpenClaw
            from tools import sessions_spawn
            
            # ACTUALLY SPAWN THE SESSION
            result = sessions_spawn(
                task=agent_prompt,
                agent_id=session_key,
                model=self.model,
                runTimeoutSeconds=task.timeout_seconds,
                cleanup="delete"  # Auto-cleanup when done
            )
            
            self.stats['agents_spawned'] += 1
            
            return {
                'success': True,
                'session_key': result.get('sessionKey') or result.get('childSessionKey') or session_key,
                'task': task,
                'spawn_time': time.time()
            }
            
        except ImportError:
            # sessions_spawn not available - not running in OpenClaw
            return {
                'success': False,
                'error': 'sessions_spawn not available. Must run inside OpenClaw session.',
                'session_key': None
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'session_key': None
            }
    
    def _collect_results_from_sessions(self, sessions: List[Dict]) -> Dict[str, AgentResult]:
        """
        Collect results from spawned sessions by checking their history.
        
        In a real implementation, this would:
        1. Poll sessions_list for status
        2. Call sessions_history to get transcripts
        3. Parse the JSON output from agent responses
        """
        results = {}
        
        for session in sessions:
            if not session.get('success'):
                # Failed to spawn
                task = session.get('task')
                result = AgentResult(
                    task_id=task.task_id if task else 'unknown',
                    agent_type=task.agent_type if task else 'unknown',
                    status=AgentStatus.FAILED,
                    output=None,
                    execution_time=0,
                    timestamp=datetime.now().isoformat(),
                    error=session.get('error', 'Unknown spawn error')
                )
                results[result.task_id] = result
                self.stats['agents_failed'] += 1
                continue
            
            # Successfully spawned - in a real implementation we'd poll for results
            # For now, mark as running
            task = session['task']
            self.active_sessions[session['session_key']] = {
                'task': task,
                'spawn_time': session['spawn_time'],
                'status': AgentStatus.RUNNING
            }
        
        return results
    
    def run_parallel(
        self,
        tasks: List[AgentTask],
        callback: Optional[Callable[[AgentResult], None]] = None
    ) -> Dict[str, AgentResult]:
        """
        Run multiple AI agents in parallel using REAL session spawning.
        
        This ACTUALLY spawns AI sub-sessions. Each is a real AI instance.
        
        Args:
            tasks: List of AgentTask objects to execute
            callback: Optional callback as each agent completes
        
        Returns:
            Dictionary of task_id -> AgentResult
        """
        if not tasks:
            return {}
        
        print(f"\nüéØ SPAWNING {len(tasks)} REAL AI AGENTS")
        print(f"   Model: {self.model}")
        print(f"   Max concurrent: {self.max_concurrent}")
        print(f"   This creates ACTUAL AI sub-sessions!\n")
        
        start_time = time.time()
        
        # Check if we're in OpenClaw environment
        try:
            from tools import sessions_spawn
            in_openclaw = True
        except ImportError:
            in_openclaw = False
        
        if not in_openclaw:
            print("‚ö†Ô∏è  WARNING: Not running inside OpenClaw session!")
            print("   sessions_spawn tool not available.")
            print("   Run this inside an OpenClaw session for real AI agents.\n")
            
            # Return error results for all tasks
            for task in tasks:
                result = AgentResult(
                    task_id=task.task_id,
                    agent_type=task.agent_type,
                    status=AgentStatus.FAILED,
                    output=None,
                    execution_time=0,
                    timestamp=datetime.now().isoformat(),
                    error='Not running in OpenClaw session. sessions_spawn unavailable.'
                )
                self.results[task.task_id] = result
            
            return self.results
        
        # Spawn all agents (up to max_concurrent at a time)
        print("üöÄ Spawning agents...")
        spawned_sessions = []
        
        for i, task in enumerate(tasks):
            if i >= self.max_concurrent:
                print(f"   (Queueing {task.agent_type} - max concurrent reached)")
                continue
            
            print(f"   Spawning {task.agent_type}...")
            session = self._spawn_agent_session(task)
            spawned_sessions.append(session)
            
            if session.get('success'):
                print(f"   ‚úÖ {task.agent_type} spawned: {session['session_key'][:40]}...")
            else:
                print(f"   ‚ùå {task.agent_type} failed: {session.get('error', 'Unknown')}")
        
        print(f"\n‚úÖ Spawned {len([s for s in spawned_sessions if s.get('success')])}/{len(tasks)} agents")
        print("‚è≥ Waiting for results...")
        print("-" * 70)
        
        # Collect results
        results = self._collect_results_from_sessions(spawned_sessions)
        
        # Calculate execution time
        total_time = time.time() - start_time
        self.stats['total_time'] = total_time
        
        print(f"\n{'='*70}")
        print(f"üìä SPAWNING COMPLETE")
        print(f"{'='*70}")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Agents spawned: {self.stats['agents_spawned']}")
        print(f"   Check sessions_list for running agents")
        print(f"{'='*70}\n")
        
        print("üìã NOTE: Agents are now running in separate sessions.")
        print("   To see results:")
        print("   1. Use sessions_list() to check status")
        print("   2. Use sessions_history() to get outputs")
        print("   3. Agents return JSON in their final message\n")
        
        return results
    
    def get_active_sessions(self) -> Dict[str, Any]:
        """Get currently running agent sessions."""
        return self.active_sessions.copy()
    
    def save_results(self, filename: Optional[str] = None) -> Path:
        """Save results to JSON file."""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"agent_results_{timestamp}.json"
        
        filepath = self.results_dir / filename
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'results': {tid: r.to_dict() for tid, r in self.results.items()}
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        
        print(f"üíæ Results saved to: {filepath}")
        return filepath


# =============================================================================
# CONVENIENCE FUNCTIONS
# =============================================================================

def create_content_team(topic: str, platform: str = 'bluesky') -> List[AgentTask]:
    """Create a team of content writing agents for a topic."""
    return [
        AgentTask('content_writer_creative', f'Write creative post about {topic}', 
                  {'topic': topic, 'platform': platform}),
        AgentTask('content_writer_funny', f'Write funny post about {topic}',
                  {'topic': topic, 'platform': platform}),
        AgentTask('content_writer_educational', f'Write educational post about {topic}',
                  {'topic': topic, 'platform': platform}),
        AgentTask('content_writer_trendy', f'Write trendy post about {topic}',
                  {'topic': topic, 'platform': platform}),
        AgentTask('content_writer_controversial', f'Write thought-provoking post about {topic}',
                  {'topic': topic, 'platform': platform}),
    ]


def create_dev_team(app_name: str, features: List[str]) -> List[AgentTask]:
    """Create a full-stack development team."""
    return [
        AgentTask('frontend_developer', f'Build frontend for {app_name}',
                  {'app_name': app_name, 'features': features}),
        AgentTask('backend_developer', f'Build backend for {app_name}',
                  {'app_name': app_name, 'features': features}),
        AgentTask('database_architect', f'Design database for {app_name}',
                  {'app_name': app_name, 'features': features}),
        AgentTask('api_designer', f'Design API for {app_name}',
                  {'app_name': app_name, 'features': features}),
        AgentTask('devops_engineer', f'Create CI/CD for {app_name}',
                  {'app_name': app_name, 'features': features}),
    ]


def create_review_team(code: str) -> List[AgentTask]:
    """Create a comprehensive code review team."""
    return [
        AgentTask('code_reviewer', 'Review code quality', {'code': code}),
        AgentTask('security_reviewer', 'Review security', {'code': code}),
        AgentTask('performance_reviewer', 'Review performance', {'code': code}),
        AgentTask('accessibility_reviewer', 'Review accessibility', {'code': code}),
        AgentTask('test_engineer', 'Generate tests', {'code': code}),
    ]


def create_meta_agent_workflow(agent_purposes: List[Dict]) -> List[AgentTask]:
    """
    Create a meta-agent workflow: creators + reviewers.
    
    Args:
        agent_purposes: List of dicts with 'name', 'purpose', 'domain', 'capabilities'
    
    Returns:
        List of AgentTasks (creators + reviewers)
    """
    tasks = []
    
    # Phase 1: Agent Creators
    for purpose in agent_purposes:
        task = AgentTask(
            agent_type='agent_creator',
            task_description=f"Design a complete AI agent: {purpose['purpose']}",
            input_data={
                'agent_name': purpose['name'],
                'domain': purpose['domain'],
                'purpose': purpose['purpose'],
                'required_capabilities': purpose['capabilities']
            },
            timeout_seconds=180
        )
        tasks.append(task)
    
    # Phase 2: Design Reviewers
    for purpose in agent_purposes:
        task = AgentTask(
            agent_type='agent_design_reviewer',
            task_description=f"Review and validate the {purpose['name']} agent design",
            input_data={
                'agent_being_reviewed': purpose['name'],
                'expected_domain': purpose['domain']
            },
            timeout_seconds=120
        )
        tasks.append(task)
    
    return tasks


def create_custom_agent(agent_name: str, purpose: str, domain: str, 
                       capabilities: List[str]) -> List[AgentTask]:
    """
    Create a single custom agent with review.
    
    Returns tasks for:
    1. Agent Creator (designs the agent)
    2. Design Reviewer (validates the design)
    """
    return create_meta_agent_workflow([{
        'name': agent_name,
        'purpose': purpose,
        'domain': domain,
        'capabilities': capabilities
    }])


# =============================================================================
# DEMO
# =============================================================================

def demo_real_ai_agents():
    """
    Demonstrate REAL AI agent spawning.
    
    THIS ACTUALLY SPAWNS AI SUB-SESSIONS when run inside OpenClaw.
    """
    print("\n" + "=" * 70)
    print("ü§ñ REAL AI PARALLEL AGENTS - NUCLEAR OPTION")
    print("   Each agent = Real spawned AI session")
    print("=" * 70)
    print("\nThis creates ACTUAL AI sub-sessions via sessions_spawn.")
    print("Each spawned agent:")
    print("  ‚Ä¢ Has its own isolated session context")
    print("  ‚Ä¢ Uses the same model as the host (kimi-coding/k2p5)")
    print("  ‚Ä¢ Can run for up to 2 minutes")
    print("  ‚Ä¢ Is automatically cleaned up after completion")
    print("  ‚Ä¢ Returns structured JSON output")
    print()
    
    # Check if we're in OpenClaw
    try:
        from tools import sessions_spawn
        print("‚úÖ Running inside OpenClaw - REAL AI agents will be spawned!\n")
    except ImportError:
        print("‚ö†Ô∏è  NOT running inside OpenClaw - agents cannot be spawned.")
        print("   Run this inside an OpenClaw session to see real AI agents.\n")
        return
    
    # Create orchestrator
    orchestrator = RealAIParallelOrchestrator(max_concurrent=3)
    
    # Demo 1: Content team
    print("-" * 70)
    print("DEMO 1: Content Generation Team (3 agents)")
    print("-" * 70)
    
    content_tasks = create_content_team("Monday motivation")[:3]
    results = orchestrator.run_parallel(content_tasks)
    
    print("\n" + "=" * 70)
    print("‚úÖ DEMO COMPLETE")
    print("=" * 70)
    print("""
The agents have been spawned and are now running independently.
Each agent will:
1. Receive its system prompt and task
2. Generate content using real AI reasoning
3. Return structured JSON output
4. Auto-terminate when complete

To check results:
- Use sessions_list() to see active sessions
- Use sessions_history() to get agent outputs
- Or implement polling in your workflow
""")


if __name__ == '__main__':
    demo_real_ai_agents()
