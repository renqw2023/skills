# Sentiment Analysis for Xiaohongshu Posts

## Overview

Xiaohongshu (å°çº¢ä¹¦) posts are unstructured user-generated content. Sentiment analysis helps extract restaurant quality signals from these posts.

## Simplified Approach (Current Implementation)

The skill uses a keyword-based sentiment scoring system:

### Positive Keywords (+1)
```
å¥½åƒ, ç¾å‘³, æ¨è, å€¼å¾—, å–œæ¬¢, æ£’, èµ, å®Œç¾, æ­£å®—, æ–°é²œ,
ç¯å¢ƒå¥½, æœåŠ¡å¥½, æ€§ä»·æ¯”, å›è´­, æ‰“å¡, å¿…åƒ, å¤ªå¥½åƒäº†
```

### Negative Keywords (-1)
```
éš¾åƒ, å¤±æœ›, å·®è¯„, ä¸æ¨è, é¿é›·, è´µ, æœåŠ¡å·®, æ€åº¦å·®,
ä¸å¥½åƒ, æ²¡ä»€ä¹ˆ, æ™®é€š, ä¸€èˆ¬, å°±é‚£æ ·, ä¸ä¼šå†æ¥äº†
```

### Neutral Keywords (0)
```
è¿˜è¡Œ, å¯ä»¥, è¯•è¯•, æ„Ÿè§‰, ä¼¼ä¹, å·®ä¸å¤š
```

## Scoring Formula

```
sentiment_score = (positive_count - negative_count) / total_mentions
```

Range: -1.0 (very negative) to 1.0 (very positive)

## Example Calculation

Post: "è¿™å®¶æ—¥æ–™åº—å¤ªå¥½åƒäº†ï¼ç¯å¢ƒå¾ˆå¥½ï¼ŒæœåŠ¡ä¹Ÿå¾ˆçƒ­æƒ…ï¼Œå°±æ˜¯æœ‰ç‚¹è´µã€‚å€¼å¾—æ¨èï¼"

- Positive: å¤ªå¥½åƒäº†, å¾ˆå¥½, çƒ­æƒ…, å€¼å¾—, æ¨è (5)
- Negative: è´µ (1)
- Total mentions: 6

```
sentiment_score = (5 - 1) / 6 = 0.67
```

Result: Moderately positive (0.67)

## Advanced Approaches (Future Enhancement)

### 1. Machine Learning Models

**SnowNLP** (Chinese NLP library)
```python
from snownlp import SnowNLP

text = "è¿™å®¶é¤å…å¤ªå¥½åƒäº†ï¼"
s = SnowNLP(text)
sentiment = s.sentiments  # 0-1, higher is more positive
```

**Pros**: More accurate, understands context
**Cons**: Requires additional dependency, slower

### 2. Aspect-Based Sentiment Analysis

Extract sentiment for specific aspects:

```
Food:     å¥½åƒ, æ–°é²œ, æ­£å®— â†’ +0.8
Service:  æœåŠ¡å¥½, çƒ­æƒ… â†’ +0.6
Price:    æœ‰ç‚¹è´µ, æ€§ä»·æ¯”é«˜ â†’ -0.2
Atmosphere: ç¯å¢ƒå¾ˆå¥½ â†’ +0.7
```

Weighted average:
```
overall_sentiment = (food Ã— 0.4) + (service Ã— 0.2) +
                   (price Ã— 0.2) + (atmosphere Ã— 0.2)
```

### 3. Emoji Analysis

Xiaohongshu posts often contain emojis:

| Emoji | Sentiment | Weight |
|-------|-----------|--------|
| ğŸ˜‹, ğŸ˜, ğŸ‘ | Positive | +0.3 |
| ğŸ˜”, ğŸ˜¡, ğŸ‘ | Negative | -0.3 |
| ğŸ˜, ğŸ¤” | Neutral | 0 |

## Integration with Recommendation Score

Sentiment score is used in two ways:

### 1. Consistency Calculation
```
sentiment_alignment = (sentiment_score + 1) / 2  # Normalize to 0-1
```

Used to verify if Dianping ratings match Xiaohongshu sentiment.

### 2. Quality Filter
Posts with sentiment < 0.0 (negative) are excluded from aggregation.

## Implementation Notes

### Current State
The skill currently uses a simplified keyword-based approach for:
- Speed (no ML model loading)
- Simplicity (no heavy dependencies)
- Explainability (clear which keywords triggered)

### When to Upgrade
Consider ML-based sentiment analysis when:
- Processing >1000 posts per day
- Need higher accuracy (>85%)
- Building production system
- Have GPU resources available

### Recommended Libraries

**For Simple Scoring** (Current):
```python
# No external libraries needed
# Pure Python keyword matching
```

**For ML-Based** (Future):
```bash
pip install snownlp jieba
```

```python
from snownlp import SnowNLP
import jieba

text = "è¿™å®¶é¤å…å¤ªå¥½åƒäº†ï¼"
s = SnowNLP(text)
print(s.sentiments)  # 0.93 (very positive)

# Aspect-based (requires training)
keywords = jieba.cut(text)
# Classify each keyword into food/service/price/atmosphere
```

## Limitations

1. **Sarcasm Detection**
   - "è¿™æœåŠ¡'çœŸå¥½'å•Š" (sarcastic) â†’ Misses negativity
   - ML models also struggle with this

2. **Context Dependence**
   - "æœ‰ç‚¹è´µ" is negative but mild
   - "å¤ªè´µäº†" is strongly negative
   - Current approach treats both equally

3. **Regional Slang**
   - Xiaohongshu users use platform-specific slang
   - Slang changes over time
   - Requires periodic keyword list updates

## Testing

Validate sentiment analysis by manually scoring 20-30 posts and comparing:

```python
posts = [
    {"text": "å¤ªå¥½åƒäº†ï¼", "expected": 0.9, "actual": 0.8},
    {"text": "ä¸€èˆ¬èˆ¬å§", "expected": 0.2, "actual": 0.1},
    # ...
]

accuracy = sum(1 for p in posts
              if abs(p['expected'] - p['actual']) < 0.2) / len(posts)

print(f"Accuracy: {accuracy * 100:.1f}%")
```

Target: >75% accuracy with simplified approach

## Resources

- **SnowNLP Documentation**: https://github.com/isnowfy/snownlp
- **Jieba (Chinese Word Segmentation)**: https://github.com/fxsjy/jieba
- **Chinese Sentiment Analysis**: Research papers on social media sentiment in Chinese

**Last Updated**: 2026-02-09
