# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "google-genai>=1.0.0",
#     "rich>=13.0.0",
#     "markdown>=3.5",
# ]
# ///
"""Start, monitor, and save Gemini Deep Research interactions.

Wraps the Gemini Interactions API to launch background deep-research
tasks, poll their status, and export the final report as Markdown.
"""
from __future__ import annotations

import argparse
import hashlib
import json
import mimetypes
import os
import sys
import time
from pathlib import Path

from google import genai
from google.genai import types
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.spinner import Spinner
from rich.table import Table
from rich.text import Text

console = Console(stderr=True)

DEFAULT_AGENT = os.environ.get(
    "GEMINI_DEEP_RESEARCH_AGENT",
    "deep-research-pro-preview-12-2025",
)

# ---------------------------------------------------------------------------
# MIME type maps (duplicated from upload.py -- PEP 723 standalone scripts)
# ---------------------------------------------------------------------------

VALIDATED_MIME: dict[str, str] = {
    ".pdf": "application/pdf",
    ".xml": "application/xml",
    ".txt": "text/plain",
    ".text": "text/plain",
    ".log": "text/plain",
    ".out": "text/plain",
    ".env": "text/plain",
    ".gitignore": "text/plain",
    ".gitattributes": "text/plain",
    ".dockerignore": "text/plain",
    ".html": "text/html",
    ".htm": "text/html",
    ".md": "text/markdown",
    ".markdown": "text/markdown",
    ".mdown": "text/markdown",
    ".mkd": "text/markdown",
    ".c": "text/x-c",
    ".h": "text/x-c",
    ".java": "text/x-java",
    ".kt": "text/x-kotlin",
    ".kts": "text/x-kotlin",
    ".go": "text/x-go",
    ".py": "text/x-python",
    ".pyw": "text/x-python",
    ".pyx": "text/x-python",
    ".pyi": "text/x-python",
    ".pl": "text/x-perl",
    ".pm": "text/x-perl",
    ".t": "text/x-perl",
    ".pod": "text/x-perl",
    ".lua": "text/x-lua",
    ".erl": "text/x-erlang",
    ".hrl": "text/x-erlang",
    ".tcl": "text/x-tcl",
    ".bib": "text/x-bibtex",
    ".diff": "text/x-diff",
}

TEXT_FALLBACK_EXTENSIONS: set[str] = {
    ".js", ".mjs", ".cjs", ".jsx",
    ".ts", ".mts", ".cts", ".tsx",
    ".json", ".jsonc", ".json5",
    ".css", ".scss", ".sass", ".less", ".styl",
    ".vue", ".svelte", ".astro",
    ".sh", ".bash", ".zsh", ".fish", ".ksh",
    ".bat", ".cmd", ".ps1", ".psm1",
    ".yaml", ".yml", ".toml", ".ini", ".cfg", ".conf",
    ".properties", ".editorconfig", ".prettierrc",
    ".eslintrc", ".babelrc", ".npmrc",
    ".rb", ".php", ".rs", ".swift", ".scala", ".clj",
    ".ex", ".exs", ".hs", ".ml", ".fs", ".fsx",
    ".r", ".jl", ".nim", ".zig", ".dart",
    ".coffee", ".elm", ".v", ".cr", ".groovy",
    ".gradle", ".cmake", ".makefile", ".mk",
    ".dockerfile", ".tf", ".hcl",
    ".sql", ".graphql", ".gql", ".proto",
    ".csv", ".tsv", ".rst", ".adoc", ".tex", ".latex",
    ".sbt", ".pom",
}

BINARY_EXTENSIONS: set[str] = {
    ".exe", ".dll", ".so", ".dylib", ".a", ".lib",
    ".zip", ".tar", ".gz", ".bz2", ".7z", ".rar", ".xz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".svg", ".webp",
    ".mp3", ".mp4", ".wav", ".avi", ".mkv", ".mov", ".flac", ".ogg",
    ".class", ".pyc", ".pyo", ".o", ".obj",
    ".wasm", ".bin", ".dat",
    ".ttf", ".otf", ".woff", ".woff2", ".eot",
}

# ---------------------------------------------------------------------------
# Pricing estimates (heuristic -- Gemini API does not return token counts)
# ---------------------------------------------------------------------------

_PRICE_ESTIMATES = {
    "embedding_per_1m_tokens": 0.15,       # gemini-embedding-001
    "pro_input_per_1m_tokens": 2.00,        # Gemini Pro <=200k context
    "pro_output_per_1m_tokens": 12.00,      # Gemini Pro <=200k context
    "chars_per_token": 4,                   # rough estimate for English text
    "research_base_input_tokens": 250_000,  # typical deep research input
    "research_base_output_tokens": 60_000,  # typical deep research output
    "research_grounded_multiplier": 1.3,    # grounded research uses ~30% more tokens
}

# ---------------------------------------------------------------------------
# Prompt templates -- concise prefixes for domain-specific research queries
# ---------------------------------------------------------------------------

_PROMPT_TEMPLATES: dict[str, str] = {
    "typescript": (
        "You are analyzing a TypeScript/JavaScript codebase. Focus on: "
        "API patterns and endpoint definitions, type signatures and interfaces, "
        "module structure and import/export graphs, monorepo layout and workspace "
        "configuration, package.json dependencies and scripts, framework-specific "
        "patterns (React components/hooks, Next.js app/pages routing, Express "
        "middleware chains, NestJS modules/providers). Pay attention to tsconfig "
        "paths, barrel exports, and type-level programming. Note any build tools "
        "(webpack, vite, esbuild, turbopack) and testing frameworks in use."
    ),
    "python": (
        "You are analyzing a Python codebase. Focus on: module structure and "
        "package layout, class hierarchies and inheritance patterns, decorator "
        "usage and metaprogramming, dependency management (pyproject.toml, "
        "setup.py, requirements.txt, poetry.lock), framework-specific patterns "
        "(FastAPI routes/dependencies, Django models/views/urls, Flask blueprints, "
        "SQLAlchemy models). Pay attention to type hints, abstract base classes, "
        "entry points, and CLI definitions. Note any build/task tools (setuptools, "
        "hatch, pdm, uv) and testing frameworks (pytest, unittest) in use."
    ),
    "general": "",
}

_TS_JS_EXTENSIONS: set[str] = {".ts", ".tsx", ".js", ".jsx", ".mts", ".cts", ".mjs", ".cjs"}
_PYTHON_EXTENSIONS: set[str] = {".py", ".pyw", ".pyx", ".pyi"}


_SKIP_DIRS: set[str] = {"__pycache__", "node_modules", ".git", ".tox", ".mypy_cache",
                         ".pytest_cache", "dist", "build", ".next", ".nuxt"}


def _detect_prompt_template(context_path: Path) -> str:
    """Auto-detect the best prompt template by scanning source file extensions."""
    ts_js_count = 0
    python_count = 0
    total = 0
    for p in context_path.rglob("*"):
        if not p.is_file():
            continue
        # Skip common build/cache directories
        if any(part in _SKIP_DIRS for part in p.parts):
            continue
        ext = p.suffix.lower()
        # Skip binary artifacts -- they are not source files
        if ext in BINARY_EXTENSIONS:
            continue
        if ext in _TS_JS_EXTENSIONS:
            ts_js_count += 1
            total += 1
        elif ext in _PYTHON_EXTENSIONS:
            python_count += 1
            total += 1
        elif ext:
            total += 1
    if total == 0:
        return "general"
    if ts_js_count / total > 0.5:
        return "typescript"
    if python_count / total > 0.5:
        return "python"
    return "general"


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _resolve_mime(filepath: Path) -> str | None:
    """Return MIME type for a file, or None if unsupported."""
    ext = filepath.suffix.lower()
    name_lower = filepath.name.lower()
    if name_lower in (".gitignore", ".gitattributes", ".dockerignore",
                       ".editorconfig", ".prettierrc", ".eslintrc",
                       ".babelrc", ".npmrc", ".env"):
        return VALIDATED_MIME.get(name_lower, "text/plain")
    if ext in VALIDATED_MIME:
        return VALIDATED_MIME[ext]
    if ext in TEXT_FALLBACK_EXTENSIONS:
        return "text/plain"
    if ext in BINARY_EXTENSIONS:
        return None
    guessed, _ = mimetypes.guess_type(str(filepath))
    if guessed and guessed.startswith("text/"):
        return "text/plain"
    return None


def _file_hash(filepath: Path) -> str:
    """Compute SHA-256 hash of a file for smart-sync."""
    h = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def _collect_files(
    root: Path,
    extensions: set[str] | None = None,
) -> list[Path]:
    """Recursively collect uploadable files from a directory."""
    files: list[Path] = []
    for p in sorted(root.rglob("*")):
        if not p.is_file():
            continue
        if extensions and p.suffix.lower() not in extensions:
            continue
        if _resolve_mime(p) is not None:
            files.append(p)
    return files


def get_api_key() -> str:
    """Resolve the API key from environment variables."""
    for var in ("GEMINI_DEEP_RESEARCH_API_KEY", "GOOGLE_API_KEY", "GEMINI_API_KEY"):
        key = os.environ.get(var)
        if key:
            return key
    console.print("[red]Error:[/red] No API key found.")
    console.print("Set one of: GEMINI_DEEP_RESEARCH_API_KEY, GOOGLE_API_KEY, GEMINI_API_KEY")
    sys.exit(1)


def get_client() -> genai.Client:
    """Create an authenticated GenAI client."""
    return genai.Client(api_key=get_api_key())


def get_state_path() -> Path:
    return Path(".gemini-research.json")


def load_state() -> dict:
    path = get_state_path()
    if not path.exists():
        return {"researchIds": [], "fileSearchStores": {}, "uploadOperations": {}}
    try:
        return json.loads(path.read_text())
    except (json.JSONDecodeError, OSError):
        return {"researchIds": [], "fileSearchStores": {}, "uploadOperations": {}}


def save_state(state: dict) -> None:
    get_state_path().write_text(json.dumps(state, indent=2) + "\n")


def add_research_id(interaction_id: str) -> None:
    """Track a research interaction ID in workspace state."""
    state = load_state()
    ids = state.setdefault("researchIds", [])
    if interaction_id not in ids:
        ids.append(interaction_id)
        save_state(state)


def record_research_completion(
    interaction_id: str, duration: int, grounded: bool,
) -> None:
    """Record a completed research run for adaptive polling."""
    state = load_state()
    history = state.setdefault("researchHistory", [])
    history.append({
        "id": interaction_id,
        "duration_seconds": duration,
        "grounded": grounded,
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    })
    # Keep last 50 entries to prevent unbounded growth
    state["researchHistory"] = history[-50:]
    save_state(state)


def _percentile(sorted_values: list[float], p: float) -> float:
    """Compute the p-th percentile (0-100) of a sorted list of values."""
    if not sorted_values:
        return 0.0
    k = (len(sorted_values) - 1) * (p / 100.0)
    f = int(k)
    c = f + 1
    if c >= len(sorted_values):
        return sorted_values[-1]
    return sorted_values[f] + (k - f) * (sorted_values[c] - sorted_values[f])


def _estimate_context_cost(context_path: Path, extensions: set[str] | None = None) -> dict:
    """Estimate the cost of uploading context files."""
    if context_path.is_file():
        files = [context_path] if _resolve_mime(context_path) else []
    elif context_path.is_dir():
        files = _collect_files(context_path, extensions)
    else:
        return {"files": 0, "total_bytes": 0, "estimated_tokens": 0, "estimated_cost_usd": 0.0}

    total_bytes = sum(f.stat().st_size for f in files)
    estimated_tokens = total_bytes // _PRICE_ESTIMATES["chars_per_token"]
    cost = (estimated_tokens / 1_000_000) * _PRICE_ESTIMATES["embedding_per_1m_tokens"]

    return {
        "files": len(files),
        "total_bytes": total_bytes,
        "estimated_tokens": estimated_tokens,
        "estimated_cost_usd": round(cost, 4),
    }


def _estimate_research_cost(grounded: bool, history: list[dict] | None = None) -> dict:
    """Estimate the cost of a research query based on history or defaults."""
    P = _PRICE_ESTIMATES

    # Try to refine from history
    basis = "default_estimate"
    input_tokens = P["research_base_input_tokens"]
    output_tokens = P["research_base_output_tokens"]

    if history:
        matching = [
            e for e in history
            if e.get("grounded", False) == grounded
            and isinstance(e.get("duration_seconds"), (int, float))
        ]
        if len(matching) >= 3:
            # Use duration as a rough proxy for token usage:
            # longer research -> more search iterations -> more tokens
            avg_duration = sum(e["duration_seconds"] for e in matching) / len(matching)
            # Scale tokens relative to a baseline of 120 seconds
            scale = max(0.5, avg_duration / 120.0)
            input_tokens = int(P["research_base_input_tokens"] * scale)
            output_tokens = int(P["research_base_output_tokens"] * scale)
            basis = "historical_average"

    if grounded:
        input_tokens = int(input_tokens * P["research_grounded_multiplier"])

    input_cost = (input_tokens / 1_000_000) * P["pro_input_per_1m_tokens"]
    output_cost = (output_tokens / 1_000_000) * P["pro_output_per_1m_tokens"]

    return {
        "estimated_input_tokens": input_tokens,
        "estimated_output_tokens": output_tokens,
        "estimated_cost_usd": round(input_cost + output_cost, 4),
        "basis": basis,
    }


def _estimate_usage_from_output(
    report_text: str,
    duration_seconds: int,
    grounded: bool,
    context_files: int = 0,
    context_bytes: int = 0,
    source_count: int = 0,
) -> dict:
    """Build post-run usage metadata from actual output data."""
    P = _PRICE_ESTIMATES
    output_bytes = len(report_text.encode("utf-8"))
    estimated_output_tokens = output_bytes // P["chars_per_token"]

    # Estimate input tokens from duration (same heuristic as dry-run)
    scale = max(0.5, duration_seconds / 120.0)
    estimated_input_tokens = int(P["research_base_input_tokens"] * scale)
    if grounded:
        estimated_input_tokens = int(estimated_input_tokens * P["research_grounded_multiplier"])

    input_cost = (estimated_input_tokens / 1_000_000) * P["pro_input_per_1m_tokens"]
    output_cost = (estimated_output_tokens / 1_000_000) * P["pro_output_per_1m_tokens"]
    context_tokens = context_bytes // P["chars_per_token"]
    context_cost = (context_tokens / 1_000_000) * P["embedding_per_1m_tokens"]
    total_cost = input_cost + output_cost + context_cost

    return {
        "disclaimer": "Estimates based on output size and pricing heuristics. Actual billing may differ.",
        "output_bytes": output_bytes,
        "estimated_output_tokens": estimated_output_tokens,
        "estimated_input_tokens": estimated_input_tokens,
        "estimated_cost_usd": round(total_cost, 4),
        "context_files_uploaded": context_files,
        "context_bytes_uploaded": context_bytes,
        "source_urls_found": source_count,
    }


def _get_adaptive_poll_interval(
    elapsed: float, history: list[dict], grounded: bool,
) -> float:
    """Return poll interval based on historical completion times.

    Adapts the polling frequency so that we poll most aggressively during the
    window where research is most likely to finish (p25-p75 of past durations).
    Falls back to the fixed curve when insufficient history exists (<3 points).
    """
    # Filter history by grounded / non-grounded
    durations = sorted(
        entry["duration_seconds"]
        for entry in history
        if entry.get("grounded", False) == grounded
        and isinstance(entry.get("duration_seconds"), (int, float))
    )

    # Need at least 3 data points to build a meaningful distribution
    if len(durations) < 3:
        return _get_poll_interval(elapsed)

    min_d = durations[0]
    p25 = _percentile(durations, 25)
    p75 = _percentile(durations, 75)
    max_d = durations[-1]

    if elapsed < min_d:
        # Nothing ever finishes this fast -- poll slowly
        interval = 30.0
    elif elapsed < p25:
        # Some finish here -- moderate polling
        interval = 15.0
    elif elapsed <= p75:
        # Most likely completion window -- aggressive polling
        interval = 5.0
    elif elapsed <= max_d:
        # Tail end -- moderate
        interval = 15.0
    elif elapsed <= max_d * 1.5:
        # Past longest ever but within 1.5x -- slow down
        interval = 30.0
    else:
        # Unusually long -- very slow
        interval = 60.0

    # Clamp to [2, 120] seconds as fail-safe
    return max(2.0, min(120.0, interval))


def _write_output_dir(
    output_dir: str,
    interaction_id: str,
    interaction: object,
    report_text: str,
    duration_seconds: int | None = None,
    usage: dict | None = None,
    fmt: str = "md",
) -> dict:
    """Write research results to a structured directory and return a compact summary."""
    base = Path(output_dir)
    research_dir = base / f"research-{interaction_id[:12]}"
    research_dir.mkdir(parents=True, exist_ok=True)

    # Write report.md (always kept as canonical markdown)
    report_path = research_dir / "report.md"
    report_path.write_text(report_text)

    # Write converted format file when format is not md
    if fmt != "md":
        converted_name = f"report.{fmt}"
        _convert_report(report_text, fmt, str(research_dir / converted_name))

    # Build interaction data
    outputs_data = []
    sources: list[str] = []
    if interaction.outputs:
        for i, output in enumerate(interaction.outputs):
            text = getattr(output, "text", None)
            entry: dict = {"index": i, "text": text}
            outputs_data.append(entry)
            # Try to extract URLs from the text as sources
            if text:
                import re
                urls = re.findall(r'https?://[^\s\)>\]"\']+', text)
                sources.extend(urls)

    # Write interaction.json
    interaction_data = {
        "id": interaction_id,
        "status": getattr(interaction, "status", "unknown"),
        "outputCount": len(outputs_data),
        "outputs": outputs_data,
    }
    (research_dir / "interaction.json").write_text(
        json.dumps(interaction_data, indent=2, default=str) + "\n"
    )

    # Write sources.json (deduplicated)
    seen: set[str] = set()
    unique_sources: list[str] = []
    for url in sources:
        if url not in seen:
            seen.add(url)
            unique_sources.append(url)
    (research_dir / "sources.json").write_text(
        json.dumps(unique_sources, indent=2) + "\n"
    )

    # Write metadata.json
    metadata = {
        "id": interaction_id,
        "status": getattr(interaction, "status", "unknown"),
        "report_file": str(report_path),
        "report_size_bytes": len(report_text.encode("utf-8")),
        "output_count": len(outputs_data),
        "source_count": len(unique_sources),
    }
    if duration_seconds is not None:
        metadata["duration_seconds"] = duration_seconds
    if usage is not None:
        metadata["usage"] = usage
    (research_dir / "metadata.json").write_text(
        json.dumps(metadata, indent=2) + "\n"
    )

    # Build compact stdout summary (< 500 chars)
    summary_text = report_text[:200].replace("\n", " ").strip()
    if len(report_text) > 200:
        summary_text += "..."
    compact = {
        "id": interaction_id,
        "status": getattr(interaction, "status", "unknown"),
        "output_dir": str(research_dir),
        "report_file": str(report_path),
        "report_size_bytes": len(report_text.encode("utf-8")),
        "summary": summary_text,
    }
    if duration_seconds is not None:
        compact["duration_seconds"] = duration_seconds
    if usage is not None and "estimated_cost_usd" in usage:
        compact["estimated_cost_usd"] = usage["estimated_cost_usd"]

    return compact


def resolve_store_name(name_or_alias: str) -> str:
    """Resolve a store display name to its resource name via state, or pass through."""
    if name_or_alias.startswith("fileSearchStores/"):
        return name_or_alias
    state = load_state()
    stores = state.get("fileSearchStores", {})
    if name_or_alias in stores:
        return stores[name_or_alias]
    return name_or_alias

# ---------------------------------------------------------------------------
# Report format conversion
# ---------------------------------------------------------------------------

_HTML_TEMPLATE = """\
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Research Report</title>
<style>
  body {{
    background: #1e1e2e;
    color: #cdd6f4;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    line-height: 1.6;
    max-width: 860px;
    margin: 2rem auto;
    padding: 0 1.5rem;
  }}
  h1, h2, h3, h4, h5, h6 {{ color: #89b4fa; }}
  a {{ color: #89dceb; }}
  code {{
    background: #313244;
    padding: 0.15em 0.3em;
    border-radius: 4px;
    font-family: "Fira Code", "Cascadia Code", Consolas, monospace;
    font-size: 0.9em;
  }}
  pre {{
    background: #313244;
    padding: 1em;
    border-radius: 6px;
    overflow-x: auto;
  }}
  pre code {{ background: none; padding: 0; }}
  blockquote {{
    border-left: 3px solid #585b70;
    margin-left: 0;
    padding-left: 1em;
    color: #a6adc8;
  }}
  table {{
    border-collapse: collapse;
    width: 100%;
    margin: 1em 0;
  }}
  th, td {{
    border: 1px solid #585b70;
    padding: 0.5em 0.75em;
    text-align: left;
  }}
  th {{ background: #313244; }}
</style>
</head>
<body>
{body}
</body>
</html>
"""


def _md_to_html(report_text: str) -> str:
    """Convert markdown text to a full HTML document."""
    import markdown as _markdown

    body = _markdown.markdown(
        report_text,
        extensions=["fenced_code", "tables", "codehilite"],
    )
    return _HTML_TEMPLATE.format(body=body)


def _convert_report(report_text: str, fmt: str, output_path: str) -> None:
    """Write *report_text* to *output_path* in the requested format."""
    if fmt == "md":
        Path(output_path).write_text(report_text)
    elif fmt == "html":
        Path(output_path).write_text(_md_to_html(report_text))
    elif fmt == "pdf":
        try:
            from weasyprint import HTML as _WeasyHTML  # type: ignore[import-untyped]
        except ImportError:
            print(
                "PDF export requires weasyprint: pip install weasyprint",
                file=sys.stderr,
            )
            sys.exit(1)
        html_str = _md_to_html(report_text)
        _WeasyHTML(string=html_str).write_pdf(output_path)
    else:
        Path(output_path).write_text(report_text)


# ---------------------------------------------------------------------------
# --context helpers
# ---------------------------------------------------------------------------

def _upload_context_files(
    client: genai.Client,
    context_path: Path,
    extensions: set[str] | None = None,
) -> tuple[str, int, int]:
    """Create an ephemeral store, upload files from *context_path*.

    Returns (store_resource_name, file_count, total_bytes).
    """
    path_hash = hashlib.sha256(str(context_path.resolve()).encode()).hexdigest()[:12]
    ts = int(time.time())
    display_name = f"context-{path_hash}-{ts}"

    store = client.file_search_stores.create(
        config={"display_name": display_name},
    )
    store_name: str = store.name
    console.print(f"Created context store: [bold]{display_name}[/bold]")

    # Collect files
    if context_path.is_file():
        if _resolve_mime(context_path) is None:
            console.print(f"[red]Error:[/red] Unsupported file type: {context_path.suffix}")
            sys.exit(1)
        files = [context_path]
    elif context_path.is_dir():
        files = _collect_files(context_path, extensions)
        if not files:
            console.print("[yellow]No uploadable files found in context path.[/yellow]")
            # Clean up the empty store
            try:
                client.file_search_stores.delete(name=store_name)
            except Exception:
                pass
            sys.exit(1)
    else:
        console.print(f"[red]Error:[/red] Context path is not a file or directory: {context_path}")
        sys.exit(1)

    console.print(f"Uploading [bold]{len(files)}[/bold] file(s) to context store...")

    # Smart-sync always on for context stores
    state = load_state()
    hash_cache: dict[str, str] = state.get("_hashCache", {}).get(store_name, {})

    uploaded = 0
    skipped = 0
    for filepath in files:
        rel = str(filepath)
        current_hash = _file_hash(filepath)
        if hash_cache.get(rel) == current_hash:
            skipped += 1
            continue
        try:
            operation = client.file_search_stores.upload_to_file_search_store(
                file=str(filepath),
                file_search_store_name=store_name,
                config={"display_name": filepath.name},
            )
            while not operation.done:
                time.sleep(2)
                operation = client.operations.get(operation)
            uploaded += 1
            hash_cache[rel] = current_hash
        except Exception as exc:
            console.print(f"[yellow]Warning:[/yellow] Failed to upload {filepath.name}: {exc}")

    # Persist hash cache
    state = load_state()
    state.setdefault("_hashCache", {})[store_name] = hash_cache
    save_state(state)

    console.print(f"[green]Context uploaded:[/green] {uploaded} new, {skipped} unchanged")

    total_bytes = sum(f.stat().st_size for f in files)

    # Track as ephemeral context store in state
    state = load_state()
    ctx_stores = state.setdefault("contextStores", {})
    ctx_stores[display_name] = store_name
    state.setdefault("fileSearchStores", {})[display_name] = store_name
    save_state(state)

    return store_name, len(files), total_bytes


def _cleanup_context_store(client: genai.Client, store_name: str) -> None:
    """Delete an ephemeral context store and remove it from state."""
    try:
        client.file_search_stores.delete(name=store_name)
    except Exception as exc:
        console.print(f"[yellow]Warning:[/yellow] Failed to delete context store: {exc}")
        return

    state = load_state()
    # Remove from contextStores
    ctx_stores = state.get("contextStores", {})
    to_remove = [k for k, v in ctx_stores.items() if v == store_name]
    for k in to_remove:
        del ctx_stores[k]
    # Remove from fileSearchStores
    fs_stores = state.get("fileSearchStores", {})
    to_remove = [k for k, v in fs_stores.items() if v == store_name]
    for k in to_remove:
        del fs_stores[k]
    # Remove hash cache
    hc = state.get("_hashCache", {})
    if store_name in hc:
        del hc[store_name]
    save_state(state)
    console.print(f"[green]Context store cleaned up.[/green]")


# ---------------------------------------------------------------------------
# start subcommand
# ---------------------------------------------------------------------------

def cmd_start(args: argparse.Namespace) -> None:
    """Start a new deep research interaction."""
    client = get_client()
    query: str = args.query

    # Prepend report format if specified
    if args.report_format:
        format_map = {
            "executive_summary": "Executive Brief",
            "detailed_report": "Technical Deep Dive",
            "comprehensive": "Comprehensive Research Report",
        }
        label = format_map.get(args.report_format, args.report_format)
        query = f"[Report Format: {label}]\n\n{query}"

    # Handle follow-up: prepend context from previous interaction
    if args.follow_up:
        console.print(f"Loading previous research [bold]{args.follow_up}[/bold] for context...")
        try:
            prev = client.interactions.get(args.follow_up)
            if prev.outputs:
                prev_text = ""
                for output in prev.outputs:
                    text = getattr(output, "text", None)
                    if text:
                        prev_text = text  # use the last text output
                if prev_text:
                    query = (
                        f"[Follow-up to previous research]\n\n"
                        f"Previous findings:\n{prev_text[:4000]}\n\n"
                        f"New question:\n{query}"
                    )
        except Exception as exc:
            console.print(f"[yellow]Warning:[/yellow] Could not load previous research: {exc}")

    # Handle file attachment: upload to a temporary store
    file_search_store_names: list[str] | None = None
    context_store_name: str | None = None
    context_file_count: int = 0
    context_bytes: int = 0

    if args.store:
        file_search_store_names = [resolve_store_name(args.store)]

    # Parse --context path and extensions (needed for both dry-run and real run)
    context_path: Path | None = None
    ctx_extensions: set[str] | None = None
    if getattr(args, "context", None):
        context_path = Path(args.context).resolve()
        if not context_path.exists():
            console.print(f"[red]Error:[/red] Context path not found: {context_path}")
            sys.exit(1)
        raw_ext = getattr(args, "context_extensions", None)
        if raw_ext:
            parts: list[str] = []
            for item in raw_ext:
                parts.extend(item.replace(",", " ").split())
            ctx_extensions = {
                ext if ext.startswith(".") else f".{ext}"
                for ext in parts
                if ext.strip()
            }

    # Resolve prompt template (skip prepend for --dry-run)
    template_choice = getattr(args, "prompt_template", "auto")
    if template_choice == "auto" and context_path is not None and context_path.is_dir():
        template_choice = _detect_prompt_template(context_path)
        if template_choice != "general":
            console.print(f"[dim]Auto-detected prompt template: {template_choice}[/dim]")
    elif template_choice == "auto":
        template_choice = "general"

    if not getattr(args, "dry_run", False):
        template_prefix = _PROMPT_TEMPLATES.get(template_choice, "")
        if template_prefix:
            query = f"[Context: {template_choice} codebase]\n{template_prefix}\n\n{query}"

    # --dry-run: estimate costs and exit without starting research
    if getattr(args, "dry_run", False):
        grounded = file_search_store_names is not None or context_path is not None
        state = load_state()
        history = state.get("researchHistory", [])

        estimate: dict = {
            "type": "cost_estimate",
            "disclaimer": (
                "Estimates only. Actual costs depend on research complexity, "
                "search depth, and API pricing changes."
            ),
            "currency": "USD",
            "estimates": {},
        }

        if context_path is not None:
            ctx_est = _estimate_context_cost(context_path, ctx_extensions)
            estimate["estimates"]["context_upload"] = ctx_est

        research_est = _estimate_research_cost(grounded, history)
        estimate["estimates"]["research_query"] = research_est

        total = research_est["estimated_cost_usd"]
        if "context_upload" in estimate["estimates"]:
            total += estimate["estimates"]["context_upload"]["estimated_cost_usd"]
        estimate["estimates"]["total_estimated_cost_usd"] = round(total, 4)

        # Human-readable on stderr
        console.print("[bold]Cost Estimate[/bold] (dry run -- no research started)")
        console.print()
        if "context_upload" in estimate["estimates"]:
            ctx = estimate["estimates"]["context_upload"]
            console.print(f"  Context upload: {ctx['files']} files, "
                          f"{ctx['total_bytes']:,} bytes, "
                          f"~{ctx['estimated_tokens']:,} tokens, "
                          f"~${ctx['estimated_cost_usd']:.4f}")
        res = estimate["estimates"]["research_query"]
        console.print(f"  Research query: ~{res['estimated_input_tokens']:,} input tokens, "
                      f"~{res['estimated_output_tokens']:,} output tokens, "
                      f"~${res['estimated_cost_usd']:.4f} ({res['basis']})")
        console.print(f"  [bold]Total: ~${estimate['estimates']['total_estimated_cost_usd']:.4f}[/bold]")
        console.print()
        console.print("[dim]These are heuristic estimates. The Gemini API does not return token counts.[/dim]")

        # Machine-readable on stdout
        print(json.dumps(estimate, indent=2))
        return

    # Actually upload context files (not a dry run)
    if context_path is not None:
        context_store_name, context_file_count, context_bytes = _upload_context_files(
            client, context_path, ctx_extensions,
        )
        if file_search_store_names is None:
            file_search_store_names = []
        file_search_store_names.append(context_store_name)

    if args.file:
        filepath = Path(args.file).resolve()
        if not filepath.exists():
            console.print(f"[red]Error:[/red] File not found: {filepath}")
            sys.exit(1)
        if args.use_file_store:
            # Upload to a store for grounding
            console.print(f"Uploading [bold]{filepath.name}[/bold] to file search store...")
            store = client.file_search_stores.create(
                config={"display_name": f"research-{filepath.stem}"}
            )
            operation = client.file_search_stores.upload_to_file_search_store(
                file=str(filepath),
                file_search_store_name=store.name,
                config={"display_name": filepath.name},
            )
            while not operation.done:
                time.sleep(3)
                operation = client.operations.get(operation)
            console.print(f"[green]Uploaded to store:[/green] {store.name}")
            if file_search_store_names is None:
                file_search_store_names = []
            file_search_store_names.append(store.name)

            # Track in state
            st = load_state()
            st.setdefault("fileSearchStores", {})[f"research-{filepath.stem}"] = store.name
            save_state(st)
        else:
            # Inline file: append file contents to query (for smaller files)
            try:
                content = filepath.read_text(errors="replace")
                if len(content) > 100_000:
                    console.print(
                        "[yellow]Warning:[/yellow] File is large. "
                        "Consider using --use-file-store for better results."
                    )
                query = f"{query}\n\n---\nAttached file ({filepath.name}):\n{content}"
            except Exception as exc:
                console.print(f"[red]Error reading file:[/red] {exc}")
                sys.exit(1)

    # Build create kwargs
    create_kwargs: dict = {
        "input": query,
        "agent": DEFAULT_AGENT,
        "background": True,
    }
    if file_search_store_names:
        create_kwargs["config"] = {
            "file_search_store_names": file_search_store_names,
        }

    console.print("Starting deep research...")
    try:
        interaction = client.interactions.create(**create_kwargs)
    except Exception as exc:
        # Fallback: try without config if the SDK version doesn't support it
        if file_search_store_names and "config" in create_kwargs:
            console.print("[yellow]Note:[/yellow] Retrying without file search store config...")
            del create_kwargs["config"]
            try:
                interaction = client.interactions.create(**create_kwargs)
            except Exception as inner_exc:
                console.print(f"[red]Error:[/red] {inner_exc}")
                sys.exit(1)
        else:
            console.print(f"[red]Error:[/red] {exc}")
            sys.exit(1)

    interaction_id = interaction.id
    add_research_id(interaction_id)

    console.print(f"[green]Research started.[/green]")
    console.print(f"  ID: [bold]{interaction_id}[/bold]")
    console.print(f"  Status: {interaction.status}")
    console.print()
    console.print("Use [bold]research.py status[/bold] to check progress.")

    # If --output or --output-dir is set, poll until complete then save
    output_dir = getattr(args, "output_dir", None)
    grounded = file_search_store_names is not None
    adaptive_poll = not getattr(args, "no_adaptive_poll", False)
    keep_context = getattr(args, "keep_context", False)

    if args.output or output_dir:
        try:
            _poll_and_save(
                client, interaction_id,
                output_path=args.output,
                output_dir=output_dir,
                show_thoughts=not args.no_thoughts,
                timeout=args.timeout,
                grounded=grounded,
                adaptive_poll=adaptive_poll,
                context_files=context_file_count,
                context_bytes=context_bytes,
                fmt=getattr(args, "format", "md") or "md",
            )
        finally:
            # Clean up ephemeral context store unless --keep-context
            if context_store_name and not keep_context:
                _cleanup_context_store(client, context_store_name)
            elif context_store_name and keep_context:
                console.print(f"[dim]Context store kept:[/dim] {context_store_name}")
    else:
        # Non-blocking mode: include context store info in JSON output
        output: dict = {"id": interaction_id, "status": interaction.status}
        if context_store_name:
            output["contextStore"] = context_store_name
            if not keep_context:
                console.print(
                    "[dim]Note: Context store will not be auto-cleaned in non-blocking mode.[/dim]"
                )
                console.print(
                    f"[dim]Clean up manually: store.py delete {context_store_name}[/dim]"
                )
        print(json.dumps(output))


def _get_poll_interval(elapsed: float) -> float:
    """Return an adaptive poll interval based on elapsed time."""
    if elapsed < 30:
        return 5
    elif elapsed < 120:
        return 10
    elif elapsed < 600:
        return 30
    else:
        return 60


def _poll_and_save(
    client: genai.Client,
    interaction_id: str,
    output_path: str | None = None,
    output_dir: str | None = None,
    show_thoughts: bool = True,
    timeout: int = 1800,
    grounded: bool = False,
    adaptive_poll: bool = True,
    context_files: int = 0,
    context_bytes: int = 0,
    fmt: str = "md",
) -> None:
    """Poll until research completes, then save the report."""
    console.print("Waiting for research to complete...")

    # Load history for adaptive polling
    history: list[dict] = []
    use_adaptive = False
    if adaptive_poll:
        try:
            state = load_state()
            history = state.get("researchHistory", [])
            # Need at least 3 matching entries to use adaptive
            matching = [
                e for e in history
                if e.get("grounded", False) == grounded
                and isinstance(e.get("duration_seconds"), (int, float))
            ]
            use_adaptive = len(matching) >= 3
        except Exception:
            pass  # Silently fall back to fixed curve

    if use_adaptive:
        console.print("[dim]Using adaptive polling (based on history).[/dim]")

    prev_output_count = 0
    start_time = time.monotonic()
    with Live(Spinner("dots", text="Researching..."), console=console, refresh_per_second=4) as live:
        while True:
            elapsed = time.monotonic() - start_time
            if elapsed > timeout:
                live.update(Text(f"Timed out after {int(elapsed)}s.", style="red bold"))
                console.print(f"[red]Error:[/red] Research timed out after {int(elapsed)} seconds.")
                console.print(f"Use [bold]research.py status {interaction_id}[/bold] to check later.")
                sys.exit(1)

            try:
                interaction = client.interactions.get(interaction_id)
            except Exception as exc:
                # Transient error -- log and retry
                interval = (
                    _get_adaptive_poll_interval(elapsed, history, grounded)
                    if use_adaptive
                    else _get_poll_interval(elapsed)
                )
                live.update(Text(f"Poll error (retrying): {exc}", style="yellow"))
                time.sleep(interval)
                continue

            status = interaction.status

            if show_thoughts and interaction.outputs:
                current_count = len(interaction.outputs)
                if current_count > prev_output_count:
                    # Show new thinking steps
                    for output in interaction.outputs[prev_output_count:]:
                        text = getattr(output, "text", None)
                        if text:
                            live.update(
                                Panel(
                                    Text(text[:500] + ("..." if len(text) > 500 else ""), style="dim"),
                                    title=f"Status: {status} ({int(elapsed)}s elapsed)",
                                    subtitle=f"Step {current_count}",
                                )
                            )
                    prev_output_count = current_count

            if status == "completed":
                live.update(Text("Research complete!", style="green bold"))
                break
            elif status in ("failed", "cancelled"):
                live.update(Text(f"Research {status}.", style="red bold"))
                console.print(f"[red]Research {status}.[/red]")
                sys.exit(1)

            interval = (
                _get_adaptive_poll_interval(elapsed, history, grounded)
                if use_adaptive
                else _get_poll_interval(elapsed)
            )
            time.sleep(interval)

    duration = int(time.monotonic() - start_time)

    # Record completion for future adaptive polling
    try:
        record_research_completion(interaction_id, duration, grounded)
    except Exception:
        pass  # Non-critical -- don't fail the save over history tracking

    # Extract final report
    report_text = ""
    if interaction.outputs:
        for output in reversed(interaction.outputs):
            text = getattr(output, "text", None)
            if text:
                report_text = text
                break

    if not report_text:
        console.print("[yellow]Warning:[/yellow] No text output found in completed research.")
        return

    # Compute usage metadata
    # Count sources from the report text
    import re as _re
    source_urls = _re.findall(r'https?://[^\s\)>\]"\']+', report_text)
    seen_urls: set[str] = set()
    unique_urls: list[str] = []
    for u in source_urls:
        if u not in seen_urls:
            seen_urls.add(u)
            unique_urls.append(u)

    usage = _estimate_usage_from_output(
        report_text=report_text,
        duration_seconds=duration,
        grounded=grounded,
        context_files=context_files,
        context_bytes=context_bytes,
        source_count=len(unique_urls),
    )

    # Write to output directory if specified
    if output_dir:
        compact = _write_output_dir(
            output_dir, interaction_id, interaction, report_text, duration, usage,
            fmt=fmt,
        )
        console.print()
        console.print(f"[green]Results saved to:[/green] {compact['output_dir']}")
        if "estimated_cost_usd" in compact:
            console.print(f"[dim]Estimated cost: ~${compact['estimated_cost_usd']:.4f}[/dim]")
        print(json.dumps(compact))
        return

    # Write to single file
    if output_path:
        _convert_report(report_text, fmt, output_path)
        console.print()
        console.print(f"[green]Report saved to:[/green] {output_path}")
        if usage.get("estimated_cost_usd"):
            console.print(f"[dim]Estimated cost: ~${usage['estimated_cost_usd']:.4f}[/dim]")

# ---------------------------------------------------------------------------
# status subcommand
# ---------------------------------------------------------------------------

def cmd_status(args: argparse.Namespace) -> None:
    """Check the status of a research interaction."""
    client = get_client()
    interaction_id: str = args.research_id

    try:
        interaction = client.interactions.get(interaction_id)
    except Exception as exc:
        console.print(f"[red]Error:[/red] {exc}")
        sys.exit(1)

    # Status summary
    status = interaction.status
    style = {"completed": "green", "failed": "red", "cancelled": "red"}.get(status, "yellow")
    console.print(f"Status: [{style}]{status}[/{style}]")
    console.print(f"ID: {interaction_id}")

    # Show outputs summary
    outputs = interaction.outputs or []
    if outputs:
        console.print(f"Outputs: {len(outputs)} step(s)")
        console.print()

        for i, output in enumerate(outputs):
            text = getattr(output, "text", None)
            if text:
                label = "Final Report" if i == len(outputs) - 1 and status == "completed" else f"Step {i + 1}"
                # Truncate for display
                preview = text[:300] + ("..." if len(text) > 300 else "")
                console.print(Panel(preview, title=label))
    else:
        console.print("[dim]No outputs yet.[/dim]")

    # Machine-readable on stdout
    result: dict = {"id": interaction_id, "status": status, "outputCount": len(outputs)}
    print(json.dumps(result))

# ---------------------------------------------------------------------------
# report subcommand
# ---------------------------------------------------------------------------

def cmd_report(args: argparse.Namespace) -> None:
    """Generate and save a markdown report from a completed interaction."""
    client = get_client()
    interaction_id: str = args.research_id

    try:
        interaction = client.interactions.get(interaction_id)
    except Exception as exc:
        console.print(f"[red]Error:[/red] {exc}")
        sys.exit(1)

    if interaction.status != "completed":
        console.print(
            f"[red]Error:[/red] Interaction is not completed. "
            f"Current status: {interaction.status}"
        )
        sys.exit(1)

    outputs = interaction.outputs or []
    if not outputs:
        console.print("[red]Error:[/red] No outputs found for this interaction.")
        sys.exit(1)

    # Build markdown report from outputs
    sections: list[str] = []
    sections.append(f"# Deep Research Report\n")
    sections.append(f"**Interaction ID:** `{interaction_id}`\n")
    sections.append(f"**Status:** {interaction.status}\n")
    sections.append("---\n")

    for i, output in enumerate(outputs):
        text = getattr(output, "text", None)
        if text:
            if i == len(outputs) - 1:
                sections.append(text)
            else:
                sections.append(f"### Research Step {i + 1}\n")
                sections.append(text)
                sections.append("\n---\n")

    report = "\n".join(sections)

    fmt = getattr(args, "format", "md") or "md"
    output_dir = getattr(args, "output_dir", None)
    if output_dir:
        compact = _write_output_dir(
            output_dir, interaction_id, interaction, report, fmt=fmt,
        )
        console.print(f"[green]Results saved to:[/green] {compact['output_dir']}")
        print(json.dumps(compact))
        return

    output_path = args.output or f"research-report-{interaction_id[:8]}.{fmt}"
    _convert_report(report, fmt, output_path)
    console.print(f"[green]Report saved to:[/green] {output_path}")

# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="research",
        description="Gemini Deep Research: start, monitor, and save research interactions",
    )
    sub = parser.add_subparsers(dest="command")

    # start (default)
    start_p = sub.add_parser("start", help="Start a new deep research interaction (default)")
    start_p.add_argument("query", help="The research query or instructions")
    start_p.add_argument(
        "--file", metavar="PATH",
        help="Attach a file to the research (inlined or uploaded to store)",
    )
    start_p.add_argument(
        "--use-file-store", action="store_true",
        help="Upload attached file to a file search store for grounding",
    )
    start_p.add_argument(
        "--store", metavar="NAME",
        help="Use a pre-existing file search store for grounding (name or resource ID)",
    )
    start_p.add_argument(
        "--report-format",
        choices=["executive_summary", "detailed_report", "comprehensive"],
        help="Desired report format",
    )
    start_p.add_argument(
        "--follow-up", metavar="ID",
        help="Continue from a previous research interaction",
    )
    start_p.add_argument(
        "--output", "-o", metavar="PATH",
        help="Wait for completion and save report to this path",
    )
    start_p.add_argument(
        "--no-thoughts", action="store_true",
        help="Suppress thinking step display during polling",
    )
    start_p.add_argument(
        "--timeout", type=int, default=1800,
        help="Maximum seconds to wait when --output is used (default: 1800)",
    )
    start_p.add_argument(
        "--output-dir", metavar="DIR",
        help="Wait for completion and save structured results to this directory",
    )
    start_p.add_argument(
        "--no-adaptive-poll", action="store_true",
        help="Disable history-adaptive polling; use fixed interval curve instead",
    )
    start_p.add_argument(
        "--context", metavar="PATH",
        help="Path to file or directory for automatic RAG-grounded research (creates ephemeral store)",
    )
    start_p.add_argument(
        "--context-extensions", nargs="*", metavar="EXT",
        help="Filter context uploads by extension (comma or space separated, e.g. py,md or .py .md)",
    )
    start_p.add_argument(
        "--keep-context", action="store_true",
        help="Keep the ephemeral context store after research completes (default: auto-delete)",
    )
    start_p.add_argument(
        "--dry-run", action="store_true",
        help="Estimate costs without starting research",
    )
    start_p.add_argument(
        "--format", choices=["md", "html", "pdf"], default="md",
        help="Output format for the report (default: md)",
    )
    start_p.add_argument(
        "--prompt-template",
        choices=["typescript", "python", "general", "auto"],
        default="auto",
        help="Prompt template to prepend for domain-specific research (default: auto-detect from --context)",
    )

    # status
    status_p = sub.add_parser("status", help="Check research interaction status")
    status_p.add_argument("research_id", help="The interaction ID")

    # report
    report_p = sub.add_parser("report", help="Save a markdown report from completed research")
    report_p.add_argument("research_id", help="The interaction ID")
    report_p.add_argument("--output", "-o", metavar="PATH", help="Output file path")
    report_p.add_argument(
        "--output-dir", metavar="DIR",
        help="Save structured results to this directory",
    )
    report_p.add_argument(
        "--format", choices=["md", "html", "pdf"], default="md",
        help="Output format for the report (default: md)",
    )

    return parser


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)

    commands = {
        "start": cmd_start,
        "status": cmd_status,
        "report": cmd_report,
    }

    if args.command is None:
        # Default to start if a bare query is provided
        # Re-parse with start as default
        if argv is None:
            argv = sys.argv[1:]
        if argv and not argv[0].startswith("-") and argv[0] not in commands:
            argv = ["start"] + list(argv)
            args = parser.parse_args(argv)

    handler = commands.get(args.command)
    if handler is None:
        parser.print_help()
        sys.exit(1)
    handler(args)


if __name__ == "__main__":
    main()
